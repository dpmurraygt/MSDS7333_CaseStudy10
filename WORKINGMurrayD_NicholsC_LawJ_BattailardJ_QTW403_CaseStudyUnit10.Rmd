---
title: "Spam Detection Using RPART"
author: "Dennis Murray, Jared Law, Julien Bataillard, Cory Nichols"
date: "March 20th, 2018"
output:
  word_document:
    fig_caption: yes
section: MSDS 7333-403 - Quantifying the World - Case Study 5 (Unit 10)
---

```{r setup, include=FALSE, echo=FALSE}
dir <- "~/DataScience/SMU/QTW/Unit10/CaseStudy/"
setwd(dir)
knitr::opts_knit$set(root.dir = dir)
knitr::opts_chunk$set(echo = FALSE)
```

```{r load_libs, include=FALSE, warning=FALSE, echo=FALSE}
library(magrittr)
library(reshape2)
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(tm)
library(rpart)
library(caret)
library(mlr)
library(parallelMap)
library(doParallel)
library(missForest)
library(scales)
library(ggthemes)
library(GGally)
library(ltm)
library(rpart.plot)
library(knitr)
```

```{r make, include=FALSE, cache=TRUE, echo=FALSE}
# we have commented this block because there is a lengthy imputation
# process involved. We load the dataset directly for efficiency post imputation
# but leave the commented code to show how imputation was accomplished.

# get and clean data
#source("src/make.R")

# rename and relevel vars for better interpretation
#emailDFrp$isSpam <- emailDFrp$isSpam %>% 
#                      revalue(c("T"="Spam", "F"="Valid")) %>% 
#                        relevel("Spam")

# impute missing vals based on forest classification and regression
# set up parallel method

# this method has been cached but runs each time so we saved output and load below

#registerDoParallel(cores=4)
#df <- missForest(emailDFrp, 
#                 maxiter=5, 
#                 ntree=200, 
#                 parallelize = c('forests'),
#                 variablewise = TRUE)

# establish imputed set
#emailDFrp <- df$ximp

load('src/emailDFrp')

```

## Abstract

The use of email has grown exponentially since the introduction of the world wide web in the late 20th century. Today, spam email is ubiquotous on every email platform. Spam detection methods to filter out unwanted emails originated in the late 1990s, and while the algorithms have improved, so have spam avoidance methods. In this paper, we explore rpart; a classification and regression tree package in R. Specifically, we explore the effectiveness of spam classification using rpart and its hyperparameters on a dataset of emails previously classified as spam or valid email. After parameter optimization, our custom rpart model outperforms an rpart model using default settings.

## Introduction

Electronic mail is an integral part of everyday life. The information spread through the use of email is massive in scale, and often includes unwanted marketing and phishing. Email adoption spread rapidly in the 1980s, and so did the ability for companies to market products to email users. In addition, entities with ulterior motives sprang up, attempting to gain information from people using phishing and social engineering. These unwanted emails were given the term "spam."

Spam filters were introduced not long after the introduction of email. These filters automatically process incoming messages and apply different statistical techniques to identify and remove unwanted emails. Bayesian email filters began to be utilized in 1996 but didn’t become popular until much later. These techniques utilize the probabilities of certain words occurring in regular emails versus spam emails to determine if a message is spam or valid.

Today, many different analytical methods exist for spam email classification. One such spam filtering method is the utilization of decision trees. We explore a decision tree package in R called rpart, which is short for recursive partitioning. Our objective is to investigate and optimize key hyperparameters used in the rpart package in order to classify email messages as spam or valid email. Specifically, we fit two separate decision trees for classifying spam email using the rpart package. We fit one decision tree on training data using default hyperparameters in rpart. After optimizing the minsplit, maxdepth, complexity and splitting criteria parameters, we fit a second decision tree on the same training data.

We compare each model's generalization performance on a test dataset containing emails previously classified as spam or valid email. The metric used for optimization is AUC or the area under the ROC curve. We use this metric to maximize the true positive rate of our classifiers. However, we also consider false negative and positive rates when comparing models to determine if optimization has resulted in a better rpart model overall.

In the subsquent section, we review research literature and introduce the email dataset. In the methods section, we explore the emails dataset and explain the methods used for optimizing rpart decision tree parameters. We implement two different decision trees and compare their generalization performance to determine if hyperparameter tuning results in better performance. Our paper concludes with a discussion of the applications of the improvements in anti-spam email filtering.

## Background

In their paper from the Hawaii International Conference on System Sciences, Cukier and Cody define spam as "including all electronic messages that are unsolicited or unwanted, sent to many users (in bulk), without regard to the identity of the individual user, and usually having commercial purposes" (1). Spam also includes messages containing attachments that spread viruses through emails. They state that in 2002, spam numbers peaked at one in three email messages. Also, in 2003, approximately 20 billion spam messages were sent daily [1].

Clearly, spam has become a major problem for users, businesses, and the internet in general, which led to the introduction of spam filters. These filters have historically relied on keywords within the message to identify spam. Some of the methods used include list-based filters which classify the sender, content-based filters, challenge/response systems, and collaborative filters where users report spam messages which are stored in a database (2).

Many researchers in academia focus on creating a web spam taxonomy to prevent spam from spreading. The literature goes into the different types of spamming and the way it is used to collect information from the users. But as spam detection improves, so does the spammers' techniques to send spam (3).

There are many types of classification methods to detect spam. Statistical methods such as support vector machines, naïve bayes classifiers, and decision trees have commonly been used. The latter is the method used in this project. 

A decision tree is a graph used to model outcomes based on a given set of rules. It can be used to classify unlabeled data (4), such as the dataset of emails we use in this case study to identify spam. In our case, we utilize rpart, a recursive partitioning package in R. Recursive partitioning is a statistical method that outputs a decision tree. Regression and classification tasks can be executed using recursive partitioning. The rpart package is one of the most commonly used packages for machine learning in R. It implements the classic non-parametric CART algorithm, using the Gini index as a default splitting criteria. The package also contains a number of optional parameters which can be tuned to optimize model performance. We introduce these parameters in the methods section.

### Data Description

The dataset used for our analysis task is made up of a corpus of emails from SpamAssassin.org (Apache ). In total, there are 9348 unique emails. It contains 29 predictor variables and one response variable named isSpam. Of the 30 total variables, 17 are boolean factor variables and the remaining 13 variables are numeric variables. Each email has been previously classified as spam or valid. We will use these predictor variables and the isSpam response to create two rpart decision tree models. A high level listing of variable names is given in Table 1.

Table 1: Variable Listing
*Numeric vars - perCaps, bodyChartCt, numLines, subExcCt, subQuesCt, numAtt, numRec, hour, perHTML, subBlanks, forwards, avgWordLen, numDlr
*Boolean vars - isSpam, isRe, isYelling, underscore, priority, isinReplyTo, sortedRec, subPunc, multipartText, isPGPsigned, subSpamWords, noHost, numEnd, isOrigMsg, isDear, isWrote

For context, a sample description of five variables is given below. We omit the rest of the variable descriptions for brevity and give explanations where appropriate in future sections.

*perCaps: percentage of capitals in the email body
*isYelling: subject alpha characters are all capital
*bodyCharCt: number of characters in the body of the email
*numEnd: email ends in numbers
*isRe: subject contains reply characters Re:


## Methods

Prior to fitting decision trees using rpart, we explore the email dataset in more detail. Specifically, predictor variable relationships are examined using correlation and independence methods. Relationships of the response variable isSpam to predictor variables are also explored. In order to determine variable importance, we use rpart to extract optimal variable splits for a generic decision tree.

### Exploratory Data Analysis

A preliminary evaluation of the dataset found missing observations in 303 unique rows. The availability of values for the following predictor variables was incomplete for:
* subSpamWords
* subQuesCt
* subExcCt
* subBlanks
* numRec
* noHost
* isYelling

Rather than discard over three percent of the dataset, we undertake imputation using random forest regression and classification methods for both numeric and categorical predictor variables.

```{r dataset_bal, include=TRUE, echo=FALSE, fig.height=3.5, fig.width=8.5}
# get dataset balance
emailDFrp %>% 
  group_by(isSpam) %>% 
    dplyr::summarise(Count=n()) %>% 
      mutate(Pct = Count/sum(Count)) %>%
      ggplot(aes(x=isSpam, y=Count, label=comma(Count))) + 
      geom_bar(stat='identity') +  theme_light() + 
      ggtitle("Figure 1: Spam vs. Non-Spam Split") + 
      scale_y_continuous("Count", labels =comma, limits=c(0,7500)) + 
      scale_x_discrete("Response Variable Category")  + 
      geom_text(vjust=-0.5)
```

Our dataset is unbalanced, with 2,371 (26%) spam emails, and 6,674 (74%) valid emails. This imbalance in the dataset could introduce higher false negative rates for our analysis task. However, given the unbalance is not egregious, oversampling methods will not be introduced in this case.

#### Explanatory Variable Relationships

Considering with our imputed dataset, the correlation matrix in Figure 2 reveals several significant positive correlations between numerical predictors.

```{r cor_mat, include=TRUE, cache=TRUE, echo=FALSE, fig.height=8, fig.width=8}

# build correlation matrix
AsVector <- emailDFrp[, c(2:30)]

nums <- sapply(AsVector, is.numeric)
bools <- sapply(AsVector, is.factor)

# correlation matrix for numerical features
cormat <- (round(cor(AsVector[, nums]), 2))
cormat[lower.tri(cormat, diag=TRUE)] <- NA
cormat <- reshape2::melt(cormat, na.rm = TRUE)

cormat %>% ggplot(aes(x=Var1, y=Var2, fill=value)) + 
           geom_tile() + 
           geom_text(aes(Var1, Var2, label = value), color = "white", size = 4)+
           theme(legend.position = "bottom", 
                 axis.text.x = element_text(angle=90, 
                                            vjust=-.5),
                 legend.text=element_text(size=8)) + 
           scale_x_discrete("") + 
           scale_y_discrete("") + 
           ggtitle("Figure 2: Correlation Between Numeric Predictor Variable Pairs")
```


Three positive relationships that stand out are listed below:

* numLines and bodyCharCt
* perHTML and bodyChartCt
* numDlr and SubQuestCt

The higher correlation between predictor variables could cause these variables to be collectively overweighted in modeling, as they may not be fully independent from one another. Indeed, bodyChartCt and numLines, which represents the number of distinct lines in the body, are nearly similar variables. However, given we are using recursive partioning, collinearity issues are usually rectified naturally. The default splitting algorithm in rpart is greedy and will select the most important variable for classification if similar variables are found.

Given our dataset contains 16 boolean predictor variables, we display a Fisher's exact p-value matrix in Figure 3. This figure shows the resulting p-values for dichotomous variables only. Correlation is not an appropriate metric for analyzing nominal or dichotomous relationships. Fisher's exact test allows us to non-parametrically examine the association between our categorical variables. It also allows us to account for small numbers of observations for certain variable frequency counts.

```{r fisher_cats, include=TRUE, cache=TRUE, echo=FALSE, fig.height=8, fig.width=8}

# chi-square matrix for categorical features

Dat <- AsVector[, bools]

combos <- combn(ncol(Dat), 2)

fishers <- adply(combos, 2, function(x) {
  test <- fisher.test(Dat[, x[1]], Dat[, x[2]])

  out <- data.frame("Row" = colnames(Dat)[x[1]]
                    , "Column" = colnames(Dat[x[2]])
                    , "OddsRatio" = test$estimate
                    ,  "type"= test$alternative
                    ,  "p.value" = round(test$p.value, 2)
                    )
  return(out)

})  

fishers %>% 
  ggplot(aes(x=Row, y=Column, fill = p.value)) + 
  geom_tile() + 
  geom_text(aes(Row, Column, label = p.value), color = "white", size = 3)+
  theme(legend.position = "bottom", 
        axis.text.x = element_text(angle=90, vjust=0.5)) +
        scale_y_discrete("") + scale_x_discrete("Predictor Variable") + 
        ggtitle("Figure 3: Fisher's Exact Test for Boolean Predictor Variables")

```

Lower p-values indicate we reject the null hypothesis of random association. Obviously, significant non-random dependence between factor variables exists. This makes logical sense for variables such as isWrote, which indicates if an email is electronically scribed. Since the majority of emails are electronically scribed, we can assume this variable may not be as important for classifying spam emails. Other interesting relationships such as the independence between priority and noHost indicate that these variables may be useful separately for analysis. The variable noHost indicates a lack of a host name from a sender and priority is set by the sender of the message.

In order to be complete, we also visually inspect the biserial correlation between factors and continuous variables. Given our factors are all dichotomous nominal variables, biserial correlation is an appropriate measure to use when exploring relationships between nominal and continuous variables. Upon visual inspection, we are able to establish some common sense relationships.

```{r biser_cor, include=TRUE, echo=FALSE, cache=TRUE, fig.height=8, fig.width=8}
# get all data except response
Dat <- AsVector
# identify factor and numeric vars for biserial correlation
facs_indx <- which(lapply(AsVector, is.factor) == TRUE)
facs <- AsVector[,facs_indx]
nums <- AsVector[,-facs_indx]

# establish df of correlations
df <- as.data.frame(lapply(nums, function(x) sapply(facs, function(y) biserial.cor(x, y))))

# melt it for viz purposes
df <- reshape::melt(as.matrix(df))
df$value <- round(df$value, 2)

# plot the relationships
df %>% 
  ggplot(aes(x=X1, y=X2, fill=value)) + 
  geom_tile() + 
  geom_text(aes(X1, X2, label = value), color = "white", size = 3)+
  theme(legend.position = "bottom", 
        axis.text.x = element_text(angle=90, vjust=0.5)) +
        scale_y_discrete("") + scale_x_discrete("Categorical Predictor Variable") + 
        ggtitle("Figure 4: Biserial Correlation for Factor and Continuous Predictor Variables")

```

For example, the number of attachments (numAtt) is negatively correlated with the boolean multipartText. Multipart text messages typically do not contain attachments. Additionally, the number of forwards is negatively correlated with isInReplyTo. This makes sense as replies typically do not contain many forwards. Overall, we see stronger negative relationships when investigating the correlation between factors and continuous variables.

In summary, highly correlated predictor variables could be extraneous to our objective of predicting whether or not an email is spam. To address predictor and response relationships, we visually inspect both continuous and factor variable relationships with the isSpam response variable in the next section. Additionally, we establish variable importances using the rpart package.

#### Response Variable Relationships

As described earlier, we know that the majority of observations in the overall dataset are classified as spam. However, we can visualize the separation between spam and valid emails for factor and continuous predictors using different plotting techniques. 

First, we inspect five factor variables to determine viability for spam prediction: isRe, numEnd, subSpamWords, and isWrote. The variable numEnd indicates whether or not the "from" email prefix ends with a number. For example, "Greer5769@yahoo.com" would be classified as true for numEnd.

```{r bool_impact, include=TRUE, echo=FALSE, fig.width=8.5, fig.height=4}

emailDFrp[, c(1,which(bools)+1)] %>% 
  gather(Predictor, Value, 2:ncol(emailDFrp[,c(1, which(bools)+1)])) %>% 
    filter(Predictor %in% c("isRe", "numEnd", "subSpamWords", "isWrote")) %>%
      ggplot(aes(x=isSpam)) + 
      geom_bar() + 
      facet_grid(Value~Predictor) + 
      theme_light() + 
      ggtitle("Figure 5: Boolean Predictor Variables and Spam Outcomes", 
               subtitle = "Y Axis Faceting Shows Spam or Valid Email for Each Predictor") +       
    scale_y_continuous("Count", labels =comma) + 
    scale_x_discrete("Is Spam - True or False")
```

The variable subSpamWords is a boolean that is true when a known spam word is contained in the subject. For instance, the word "viagra" would trigger a boolean value of true for subSpamWords. The majority of spam occurs when the four factors above are set to false. On the other hand, we know that mostly valid emails occur when isRe and isWrote are set to true. There are occurences of spam emails in both boolean statuses for factor predictor variables numEnd, and subSpamWords. However, the true cases of each predictor variable have more cases of spam than valid email. Given the splits above, we can see how a decision tree could split on different categorical variables in order to classify an email message as spam or valid.

We can also identify separation of classes for numeric variables by looking at log values for each numeric predictor in a box plot. We inspect all numeric values in Figure 6.

```{r numeric_impact, include = TRUE, echo=FALSE, cache = TRUE, fig.width=8.5, fig.height=6}
nums <- which(lapply(emailDFrp, is.numeric) ==TRUE) 
emailDFrp[,c(1, nums)] %>% 
  gather(Predictor, Value, 2:ncol(emailDFrp[,c(1, nums)])) %>% 
    #filter(Predictor %in% c("forwards",   "perCaps", "perHTML")) %>%
      ggplot(aes(x=isSpam, y=log(1+Value))) + 
      geom_boxplot() + 
      facet_wrap(~Predictor, scales = "free_y", ncol=5) + 
      theme_light() + 
      ggtitle("Figure 6: Continous Predictor Variables and Spam Outcomes") + 
      scale_x_discrete("Spam or Valid Email")+
      ylab("Log Value")
```

The predictor variable forwards, which quantifies how many times an email has been forwarded, shows a more concentrated distribution of values in the third quartile for messages that are valid. The predictor variable perCaps shows a larger interquartile range for spam. We also know from the boxplot that the median perCaps value for spam messages is higher than for valid messages. Also, roughly 75 percent of valid messages have a perCaps value lower than the median perCaps value for spam messages. PerHTML, which represents the percentage of HTML in the email body, also provides for a decent classification variable. The majority of its third quartile occurs specifically with spam predictions.

Examination of these predictor variables gives some idea of how we should expect rpart to determine splits. However, we can also expect that some of the lower correlation variables might be involved in the decision of spam versus valid, perhaps providing some finer detailed distinctions between the two classes. We use rpart in the next section to determine which variables are the most important for splitting.

#### Variable Selection and Model Comparison Setup

Instead of fitting complex variable selection algorithms to our email data set, we fit an rpart model on the training data using all 29 features and default model parameters. Default parameters for rpart consist of a minsplit of 20, a complexity paramer (cp) of 0.01 and a maxdepth of 30. The Gini index is used as the splitting criterion by default.

80 percent of the email data is used for training and 20 percent is used for testing. Spam represents roughly 25 percent of the emails in our original dataset. Therefore, we stratify the observations in our training and test data sets to maintain the original spam-to-valid ratio.

We also retain this initial fit on the training data set in order to evaluate the model's generalization capabilities on the test data set. We call this model our "base" model, and use this terminology going forward. After the rpart model is trained, the package provides us with a listing of the variables it considers the most important for splitting. We can easily identify these variables with the variable importance method contained in rpart. Figure 7 shows the variables our base model considers the most important for classification.

```{r fit_dtree_base, echo=FALSE, include=TRUE, fig.height=8, fig.width=8}
set.seed(4)

# get counts to prep for train/test split
spam <- emailDFrp$isSpam == "Spam"
numSpam <- sum(spam)
numHam <- sum(!spam)

# 80/20 split, stratified
testSpamIdx <- sample(numSpam, size = floor(numSpam/5))
testHamIdx <- sample(numHam, size = floor(numHam/5))


# pull together stratified train and test sets with training 80 pct
testDF <- 
  rbind( emailDFrp[emailDFrp$isSpam == "Spam", ][testSpamIdx, ],
         emailDFrp[emailDFrp$isSpam == "Valid", ][testHamIdx, ])

trainDF <-
  rbind( emailDFrp[emailDFrp$isSpam == "Spam", ][-testSpamIdx, ], 
         emailDFrp[emailDFrp$isSpam == "Valid", ][-testHamIdx, ])


# initiate mlr classification task
# set up a learning task placeholder
spam.tsk = makeClassifTask(id = "spam", 
                           data = trainDF, 
                           target = "isSpam")

# Create the learner from embedded libraries
spam.lrn = makeLearner( cl = "classif.rpart",# use rpart algorithm, gini index is default for splitting
                        id ="spam", # give it an id
                        fix.factors.prediction = TRUE, # control for missing class if any)
                        predict.type = 'prob') # to get probabilities

# focus on maxdepth, cp and minsplit
# show defaults - cp = 0.01, minsplit=20, maxdepth=30
#getParamSet(spam.lrn) 

# fit with defaults, cp = 0.01
spam.clf <- mlr::train(spam.lrn, spam.tsk)
splits <- getLearnerModel(spam.clf)

# check out CP, default gini index
#summary(splits)

# which variables are most important?
dat <- data.frame(vars=names(splits$variable.importance), 
                  importance=splits$variable.importance)


# plot the feature importances
ggplot(dat, aes(reorder(vars, importance, sum), importance))+
    coord_flip()+
    geom_col()+
    ggtitle("Figure 7: rpart Feature Importance")+
    xlab("Feature Splits")
```

At the top of the variable importance list is perCaps, or the percentage of capital alpha characters in the body. Importance is a weighted sum of improvement in impurity for each variable split. The perCaps variable overshadows all other variables from an importance standpoint. Indeed, we saw good separation of spam and valid email in the previous section for this variable. We are able to classify 77 percent of all spam messages based on a split value of 13 percent for perCaps. In this case, the base model predicts non-spam when a message contains less than 13 percent capitals. Additionally, we see email from addresses ending in numeric (numEnd) and priority provide little value for spam classification. Base model performance and comparisons are given in the results section.

We also investigate the effect rpart's control parameters have on classifying spam emails by fitting a separate, optimized rpart model. For this model, we analyze four different parameters: complexity penalty (cp), minsplit, maxdepth, and the splitting criteria. Table 2 provides a description for each of these parameters. 

Table 2
* complexity parameter (cp)  - A scaled complexity penalty that ranges from 0 to 1. In a classification setting, cp is compared against the error rate relative to a previous split. Any split that does not decrease the overall lack of fit by cp is not considered. Default is 0.01.
* minsplit - the minimum number of observations that must exist in a node in order for a split to be attempted. Default is 20.
* maxdepth - The maximum depth of any node of the final tree, with the root node counted as depth 0. Default is 30.
* splitting criteria - gini or information. Gini utilizes the gini index to optimize split points, information uses entropy and information gain. Default is Gini.

Utilizing the full listing of variables in our email dataset can lead to overfitting, however, decision trees allow us to find the best variables for splitting while pruning extraneous variable splits. Applying the first three parameters in Table 2 typically reduces the size of the final tree. This reduction in size can help with model generalization to the test dataset. 

The rpart package contains other control parameters used primarily for exploratory purposes (xx reference). However, one point of clarification regarding the parameter xval is warranted. The parameter xval allows a user to optimize the cost penalty (cp) for a tree with k-fold cross-validation. Given rpart does not allow for the tuning of multiple parameters simultaneously, we instead rely on the mlr package (xx reference) for cross-validation and tuning. The mlr package provides a broad toolset for machine learning tasks in R. It is mainly a wrapper for other machine learning packages and provides fantastic utilities to automate tedious tasks in a machine learning workflow.

#### Hyperparameter Optimization

Optimization of parameters is achieved by exploring a discrete list of the four parameters of interest. The grid search method is used in conjunction with ten-fold stratified cross-validation. 

We seek to maximize the true positive classification rate where "spam" is our positive class. We do this by selecting area under the ROC curve (AUC) as our performance metric when tuning hyperparameters. A false positive means an important email may end up in spam or deleted. A false negative means the user may experience unfiltered messages that should be in the spam folder. We consider the former situation a more severe model error. The mean AUCs for all models and associated parameters are compared and the model with the highest cross-validated AUC is chosen as our optimized model. 

The base rpart model with default parameters is then subsequently compared to the optimized model given the test data set. We explore model performance and comparisons in the next section. We consider final model comparisons using a holistic set of performance metrics including AUC, mean misclassification error, false positive rate, and false negative rate.

## Results

### Base Model Results

Our base model lists perCaps as the most important variable to split in Figure 7. In second place is BodyCharCt, which represents the number of characters in the body of the email message. Given these variables have such a higher importance value, we visually inspect an rpart model fit on our training data set using only perCaps and BodyCharCt.

```{r perCaps_class, include=TRUE, echo=FALSE, fig.height=6, fig.width=8}

# for viz purposes, log top 2 importance, fit and predict
trainDF$perCaps <- log(1+trainDF$perCaps)
trainDF$bodyCharCt <- log(1+trainDF$bodyCharCt)

# obviously a lot of error when just using the top two predictors
# needs more to be accurate!
spam.log.tsk = makeClassifTask(id = "spam", 
                               data = trainDF, 
                               target = "isSpam")

g <- plotLearnerPrediction(learner = spam.lrn, 
                           task = spam.log.tsk, 
                           features=c("perCaps", "bodyCharCt"),
                           pointsize = 0.5, 
                           err.col="white",
                           bg.cols = c("darkblue","green"),
                           err.size = 0.5,
                           err.mark="cv")

g+
  ggtitle('Figure 8: Body Character Count and Percent Capitals Decision Tree')+
  theme(legend.key.size = unit(1, "cm"))+
  guides(shape = guide_legend(override.aes = list(size = 3)))
```

We represent values on a log scale in order to provide a clean visualization of classification regions. The observations outlined in white are misclassifications and the color boundaries represent the outcomes spam and valid. The lighter areas of the chart represent lower probabilities for a given class. Obviously, the ligher regions represent areas of higher misclassification. Additionally, utilizing only these two variables, our base model struggles with false positive rate. This indicates our model needs more complexity. However, these two variables alone provide for good separation as part of a more complex model.

Thus, we leave it up to rpart and its default parameters to create a decision tree containing 14 splits represented in Figure 9. This default rpart decision tree was created in the previous section. We fit this default model on the entire training data set.

```{r tree_plot, include=TRUE, echo=FALSE, fig.width=8.5, fig.height=5}
prp(splits, 0, extra=1)
```

Of particular note is the base model's usage of bodyCharCt. It is used three times in the splitting process for the training data. Additionally, we can see quite a bit of misclassification in the model on the training set, particularly a prevalence of false negatives. 

We utilize the test data set of emails to produce a confusion matrix and determine model generalization performance in Table 3.

```{r confusion_base, echo=FALSE, include=TRUE}

spam.preds <- predict(spam.clf, newdata =  testDF)

# get scores
preds <- as.data.frame(spam.preds$data)

# confusion matrix for default rpart
calculateConfusionMatrix(spam.preds)

#calculateConfusionMatrix(spam.pred, relative = TRUE)
performance(spam.preds, measures=list(auc, mmce, fpr, fnr))

```

In order to address overall misclassification, we include MMCE, or model misclassification error rate in our metrics set. As seen in the confusion matrix, the base model struggles with false negatives. In total, 76 observations in the test set out of 479 spam records are misclassified for a false negative rate of 15.8 percent. MMCE overall is high at almost eight percent. We attempt to improve on these metrics by optimizing rpart's hyperparameters.


### Optimized Model Results

As stated previously, we explore the complexity parameter, minimum split for each node, maximum depth of the tree, and the splitting criterion via a grid search. The parameter list is given in Table 4 below:

Parameter Search Criteria
*complexity parameter (cp) -0.001, 0.01, 0.1, 0.2, 0.5,
*minsplit - 1, 5, 10, 15, 20, 30
*maxdepth - 1, 5, 10, 15, 20, 30
*splitting criterion - gini, information

```{r rpart_optimize, echo=FALSE, cache=TRUE, include=FALSE}

# Leaving commented code to show setup of resampling method
# we load resampling binary for reproducibility
 
spam.resamp = makeResampleDesc(method = "CV", 
                               iters = 10, 
                               stratify = TRUE)

# pull out resampling instance for reproducibility
#spam.resamp.bin = makeResampleInstance(spam.resamp, task=spam.tsk)
#save(spam.resamp.bin, file ="resampling_binary") # save for future use!
load("src/resampling_binary")


# identify search grid params - we'll brute force here based on reasonable param vals
# can also random search over numeric ranges instead
spam.ps = makeParamSet(
                  makeDiscreteParam("cp", values = c(0.001, 0.01, 0.1, 0.2)),
                  makeDiscreteParam("minsplit", values = c(1, 5, 10, 15, 20, 30)),
                  makeDiscreteParam("maxdepth", values = c(1, 5, 10, 15, 20, 30)),
                  makeDiscreteParam("parms", values = list(gini = list(split = c("gini")), 
                                                           info = list(split = c("information"))))
                 )

# Create the grid and identify algorithm if needed
#spam.ctrl = makeTuneControlGrid() 

# launch parallel multicore for tuning purposes, speed
#parallelStart(mode="multicore", cpus=4)

# tune parameters for our learner, task, optimize results for auc
#spam.tuned.clf = tuneParams( learner=spam.lrn, 
#                             task=spam.tsk, 
#                             resampling=spam.resamp.bin, # compare to same resampling method!
#                             control=spam.ctrl, 
#                             par.set=spam.ps, 
#                             measures=list(auc, fpr, fnr, mmce)) # optimize for auc
#parallelStop()

# save tuned classifier to binary
#save(spam.tuned.clf, file='spam.tuned.clf')
load('src/spam.tuned.clf')

# create cleaned hyperparameter effect data from tuning
data <- generateHyperParsEffectData(spam.tuned.clf, 
                                    partial.dep = TRUE)

# get df of tuning results
df_models <- data$data

# get optimal hyperparameters for use on test
spam.opt.lrn <- setHyperPars(spam.lrn, par.vals = spam.tuned.clf$x)

# train the model on training set
spam.opt.clf <- mlr::train(spam.opt.lrn, spam.tsk)
```

Each combination of parameters is cross-validated on 10 folds of the training data set using AUC as the performance metric. After the grid search and associated cross-validations are complete, the model with the best AUC performance has a complexity penalty of 0.001, a minimum split of 10, a maximum tree node depth of 15, and uses information as its splitting criterion. We can see the optimization path for our rpart model clearly by viewing the AUC score in sorted order. 

```{r optimize_path, include=TRUE, echo=FALSE, fig.height=5, fig.width=8}

plotHyperParsEffect(data, x = "iteration", y = "auc.test.mean",
  plot.type = "line", partial.dep.learn = "regr.randomForest")+
  ggtitle("Figure 10: Optimization Path")+
  ylab("Mean AUC")

```

Optimization begins to stall just under an AUC of 0.97 for our training data. Our optimized tree is much larger than our base model. In fact, 76 total splits are used for our optimized tree. This is because the optimal complexity penalty is set lower than the default (0.01) at 0.001. This may introduce risk for overfitting our test set, which we will explore shortly.

```{r parms_look, echo=FALSE, include = TRUE}

splits <- getLearnerModel(spam.opt.clf)

```

Of the 29 models fit during the tuning process with a resulting cross-validated AUC of greater than 0.96, all have complexity parameters of 0.001. Additionally the maxdepth for a decision tree node is always 10 or higher. 14 out of the 29 models use the information splitting criterion and the majority of models have a minsplit value of 10 or higher. Optimization results favor more splits with medium node depth when compared to the default model. A single unique complexity parameter indicates dominance when it comes to maximizing AUC results for our analysis task. The splitting criterion is nearly a toss up between Gini and information criteria.

We can easily see the complexity parameter's dominance in Figure XX below, where the mean AUC across validation splits is maximized as long as the complexity parameter remains small. 

```{r cp_vs_minsplit, echo=FALSE, include=TRUE, fig.height=8, fig.width=8}
# plot hyper parameter effects
plotHyperParsEffect(data, x = "cp", y = "minsplit", z = "auc.test.mean",
  plot.type = "heatmap", partial.dep.learn = "regr.randomForest")+
  ggtitle("Figure 11: Hyperparameter Effects - Complexity and Minsplit")+
  ylab("Mean AUC")+
  guides(fill=guide_legend(title="Mean AUC"))

```

After thoroughly exploring rpart parameters, we turn to generalization performance. Given an AUC of 0.95 for our base model, we fit our optimized model to the same training data set of emails.

```{r final_results, echo=FALSE, include=TRUE}

# results
spam.opt.preds <- predict(spam.opt.clf, newdata = testDF)
spam.preds <- predict(spam.clf, newdata =  testDF)


opt<-performance(spam.opt.preds, 
            measures=list(auc, mmce, fpr, fnr))
                
base<-performance(spam.preds, 
            measures=list(auc, mmce, fpr, fnr))

kable(cbind(optimized = round(opt,3), base = round(base,3)))

```

Our optimized model outperforms the base model on our key metric: AUC. Given our base model struggled with false negative rates, we also include false positive and false negative rates for the classification of spam email on the test data set. Our model significantly improves the false negative, false positive and misclassification error rates as well. However, no analysis using AUC would be complete without viewing an ROC curve for comparison.

``` {r roc_curve, include = TRUE, echo=FALSE, fig.height=8, fig.width=8}
# plot ROC curves for base and optimized
df = generateThreshVsPerfData(list(base = spam.preds, 
                                   optimized = spam.opt.preds), 
                                   measures = list(fpr, tpr))


# plotROCCurves(df)
qplot(x = fpr, y = tpr, color = learner, data = df$data, geom = "path")+
  ggtitle("Figure 12: ROC Curves for Base and Optimized rpart Models")

```

We can now visually attest our optimized rpart model outperforms the base rpart model using default parameters. The optimized model contains more area and results in a better true positive to false positive ratio trade off.

## Conclusions and Future Work

Through detailed data exploration, extensive parameter investigation, and tuning, we are able to improve upon rpart's default settings in an email spam classification setting. We determined that the complexity penalty in rpart dominates when optimizing for AUC given our analysis task. Further, we were able to effectively generalize to a test data set using a relatively complex rpart decision tree.

Additional improvements could be achieved given a more balanced data set. Oversampling of spam emails could reduce the tendency for rpart models to give false negative results. Additionally, empirial investigation of decision thresholds and learning curves could assist in optimizing the classification criteria and amount of training data necessary to fit an optimal rpart model.

Supreme Court Justice Potter Stewart is famously credited with the quotation: "But I know it when I see it."[1] In the context that he was in, Justice Stewart was referencing a film that was thought to be outside the standards of decency accepted in the era of the 1960's. In the present day, most internet and mobile phone users could also apply it to the subject of spam, or commercial, unsolicited e-mail: they know it when they see it.

The borders of so-called "Spam" e-mail are hard to delineate versus e-mail accepted by users. It would be easy to set extremely broad borders - that might include sweeping up any commercial e-mail for disposal of, before delivery to the recipient. It would be easy as well to be too lenient - filtering only the most egregious examples, and leaving the blatant examples of spam to the refuse.

Early efforts at filtering spam were centered on the assumed validity of identity of e-mailers: recipients were expected to "white list" their favored senders, and the senders validity would be assumed valid if the e-mail was shown to be from the same address. In the last 20 years, many major e-mail handling systems have moved from this type of certainty label, to an ensemble of factors that can be labeled with a final "TRUE" or "FALSE" to the e-mail's spam content indication. Additional aide is given in creation, or validation of this level by the input predictor set.

Google, via it's gmail product, as well as other providers, have had an ample data set over the past several years. All have searched for algorithms that reduce the number of false positives (labeled as spam, when it's actually valid) and false negatives (labeled as valid, when it actually is spam).

A 2012 article in the Journal of Economic Perspectives [2] cites that up to 3% of the 50 billion pieces of spam e-mail sent each today are successful in reaching the recipient. The authors of the paper develop a cost of $20 billion annually to American consumers from spam.

The value of intercepting spam communications, across mediums, is also growing. Most mobile phone numbers have experienced the spoofed-number attack, where a number similar to your own places a call with a robot caller on the opposite end. A number of smart phone applications are currently trying to address this problem by machine learning. Likewise, SMS text messaging spam also provides a risk to users of mobile phones. In both cases, the originator of the call or message is not the number that is listed on the phone, but is a false or spoofed phone number used with the intent of deceiving the recipient. Phishing attacks via SMS skirt the boundaries of security in several ways. First, traditonally there have not been products to address it. Second, the perceived higher level of intimacy by users given to SMS versus e-mail seems to reduce the users' awareness of threats. Third, the smaller screen may reduce users' awareness to visiting spoofed websites.

As e-mail filtering continues to move in the direction of machine learning, development of new variables is likely an area for improvement in accuracy and performance. This could move further into the area of natural language processing, and perhaps more specificity to different languages. One can also imagine moving some of the service closer to the user, to further allow for customization. Of course, spam senders will also try to defeat the measures equally as much - and one can imagine training an adversarial model to allow for spam e-mail to pass through filters and land in your inbox. Engineers and developers will need to continue to walk a fine line - avoiding false positives (thereby filtering legitimate e-mails) with ensuring that users' experience and time is not absorbed by spam e-mails.


## References

http://spamassassin.apache.org

1. Nolan, D., Temple Lang, D. DATA SCIENCE IN R: a Case Studies Approach to Computational Reasoning and Problem Solving. CRC PRESS, 2017.


https://cran.r-project.org/web/packages/rpart/rpart.pdf
https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf
https://stats.stackexchange.com/questions/191842/how-does-the-complexity-parameter-correspond-to-the-number-of-splits-in-cross-va




```{r, echo=FALSE, include =FALSE, eval=FALSE}

plotHyperParsEffect(data, x = "minsplit", y = "maxdepth", z = "auc.test.mean",
  plot.type = "heatmap", partial.dep.learn = "regr.randomForest")



# plot ROC curves for base and optimized
df = generateThreshVsPerfData(list(base = spam.preds, 
                                   optimized = spam.opt.preds), 
                                   measures = list(fpr, tpr))


plotROCCurves(df)
qplot(x = fpr, y = tpr, color = learner, data = df$data, geom = "path")+
  ggtitle("test")

# what does our optimal threshold look like for our metrics?
n = getTaskSize(spam.tsk)
thresh_plot = generateThreshVsPerfData(spam.preds, measures = list(fpr, fnr, mmce))
plotThreshVsPerf(thresh_plot)+
  ggtitle("test")

```

```{r dtree_explore, echo=FALSE, include=FALSE, eval=FALSE}

# this block is exploration of the mlr package in R
# optimize both fnr and fpr
spam.ctrl = makeTuneMultiCritControlGrid()

library(emoa)
spam.tuned.clf = tuneParamsMultiCrit( learner = spam.lrn, 
                                      task = spam.tsk, 
                                      resampling = spam.resamp.bin, # compare to same resampling method!
                                      control = spam.ctrl, 
                                      par.set = spam.ps, 
                                      measures = list(fnr, fpr))

df <- as.data.frame(trafoOptPath(spam.tuned.clf$opt.path))
df[df$fnr.test.mean+df$fpr.test.mean == min(df$fnr.test.mean+df$fpr.test.mean),] # use this to fit
lrn = setHyperPars(makeLearner("classif.rpart"), par.vals = list(cp=0.001, minsplit=5, maxdepth=15))


# get a list of preds for each fold
getRRPredictionList(spam.base.clf)
# get all ze data!
pred <- getRRPredictions(spam.base.clf)
head(pred$data)

# get individual model fits, turn on models = TRUE in resample()
spam.base.clf$models

# establish train and test (learning curve here?)
# 80/20 split works best
spam.tsk = makeClassifTask(id = "spam", 
                           data = df$ximp, 
                           target = "isSpam", 
                           check.data = FALSE)

spam.lrn = makeLearner("classif.rpart",  # use rpart algorithm, gini index is default for splitting
                        id ="spam", # give it an id
                        fix.factors.prediction = TRUE, # control for missing class if any)
                        predict.type = 'prob') 

r <- generateLearningCurveData(
  learners = spam.opt.lrn,
  task = spam.tsk,
  percs = seq(0.1, 1, by = 0.1),
  measures = list(auc, setAggregation(auc, train.mean)),
  resampling = makeResampleDesc(method = "CV", iters = 10, stratify = TRUE, predict="both"),
  show.info = FALSE)

plotLearningCurve(r, facet="learner")

# try some nested resampling for unbiased error assessment
# on an outer CV, use inner to tune params, find best
# train model on outer CV splits based on these params and test on outer CV split
parallelStart(mode="multicore", cpus = 4)

tune.lrn = makeTuneWrapper(learner = spam.lrn,
                           resampling = spam.resamp, # inner resampling loop
                           par.set = spam.ps, # parameters to test
                           control = spam.ctrl, # control
                           measures = auc,
                           show.info = FALSE)

# unbiased test outer cross validation
outer = makeResampleDesc("CV", iters = 3, stratify = TRUE)

r = resample(tune.lrn, 
             spam.tsk, 
             resampling = outer, 
             extract = getTuneResult,
             measures = auc,
             show.info = FALSE)

parallelStop()

getNestedTuneResultsX(r)
data <- generateHyperParsEffectData(r, partial.dep = TRUE)
plotHyperParsEffect(data[[1]], x = "iteration", y = "auc.test.mean",
                    plot.type = "line", partial.dep.learn = "regr.randomForest")


```

```{r nbayes_get_data, echo=FALSE, include=FALSE, eval=FALSE}
# RSpamData not available in R 3.4.3

spamPath <- "Data"

dirNames <- list.files(path = spamPath, full.names = TRUE)

fileNames <- list.files(dirNames[1], full.names = TRUE)

sapply(dirNames, function(x) length(list.files(x)))

# pick random emails
idx = c(1:5, 15, 27, 68, 69, 329, 404, 427, 516, 852, 971)

#sampleEmail = sapply(list.files(dirNames[1], full.names = TRUE)[idx], readLines)

# split into header and body
splitMessage <- function(msg) {
  splitPoint = match("", msg) # find first blank line
  header = msg[1:(splitPoint-1)] # get header, will use this for attachments
  body = msg[ -(1:splitPoint) ] # get body
  return(list(header = header, body = body))
}

sampleSplit <- lapply(sampleEmail, splitMessage)

# get headers
headerList = lapply(sampleSplit, function(msg) msg$header)

# get boundary id from multi-part email

getBoundary<- function(header) {
  boundaryIdx = grep("boundary=", header) # index boundary
  boundary = gsub('"', "", header[boundaryIdx]) # replace quotes
  gsub(".*boundary= *([^;]*);?.*", "\\1", boundary) # regexrepl with first group match
}

# pass boundary and body of each message
dropAttach = function(body, boundary){
  bString = paste("--", boundary, sep = "") 
  bStringLocs = which(bString == body)# find start locs for each section of body
  # -- is arbitrary
  
  if (length(bStringLocs) <= 1) return(body) # if no boundaries, give me back body
  
  eString = paste("--", boundary, "--", sep = "") # ending boundary string
  eStringLoc = which(eString == body) # ending string loc
  if (length(eStringLoc) == 0)  # if no ending string return first part of body
    return(body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1)])
  
  # otherwise get length of body
  n = length(body)
  # if ending boundary string < rows, return body + ending rows after ending boundary
  if (eStringLoc < n) 
     return( body[ c( (bStringLocs[1] + 1) : (bStringLocs[2] - 1), 
                    ( (eStringLoc + 1) : n )) ] )
  
  # finally, return body if no other conditions met
  return( body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1) ])
}

# clean up punctuation, numbers, spaces
cleanText =
function(msg)   {
  tolower(gsub("[[:punct:]0-9[:space:][:blank:]]+", " ", msg))
}

findMsgWords = 
function(msg, stopWords) {
 if(is.null(msg))
  return(character())

  # split and return vector of cleaned words
 words = unique(unlist(strsplit(cleanText(msg), "[[:blank:]\t]+")))
 
 # drop empty and 1 letter words
 words = words[ nchar(words) > 1]
 words = words[ !( words %in% stopWords) ]
 invisible(words)
}

processAllWords = function(dirName, stopWords)
{
  # read all files in a given directory
  fileNames = list.files(dirName, full.names = TRUE)
  # drop files that are not email
  notEmail = grep("cmds$", fileNames)
  
  if ( length(notEmail) > 0) fileNames = fileNames[ - notEmail ]

  # read all messages into a list
  messages = lapply(fileNames, readLines, encoding = "latin1")
  
  # split header and body for each message
  emailSplit = lapply(messages, splitMessage)
  # put body and header in own lists
  bodyList = lapply(emailSplit, function(msg) msg$body)
  headerList = lapply(emailSplit, function(msg) msg$header)
  # save some memory, kill emailSplit var
  rm(emailSplit)
  
  # determine which messages have attachments
  hasAttach = sapply(headerList, function(header) {
    CTloc = grep("Content-Type", header)
    if (length(CTloc) == 0) return(0)
    multi = grep("multi", tolower(header[CTloc])) 
    if (length(multi) == 0) return(0)
    multi # has attachment
  })
  
  # get indices for which messages have attachments
  hasAttach = which(hasAttach > 0)
  
  # find boundary strings for messages with attachments
  boundaries = sapply(headerList[hasAttach], getBoundary)
  
  # drop attachments from message bodies
  bodyList[hasAttach] = mapply(dropAttach, bodyList[hasAttach], 
                               boundaries, SIMPLIFY = FALSE)
  
  # extract words from body
  msgWordsList = lapply(bodyList, findMsgWords, stopWords)
  
  invisible(msgWordsList)
}

msgWordsList = lapply(dirNames, processAllWords, stopWords=stopWords)

numMsgs = sapply(msgWordsList, length)

# identify which messages are spam based on num Msgs
isSpam = rep(c(FALSE, FALSE, FALSE, TRUE, TRUE), numMsgs)

# list of vectors of cleaned words for each message
msgWordsList = unlist(msgWordsList, recursive = FALSE)
```

``` {r nbayes_train_test_split, echo=FALSE, include=FALSE, eval=FALSE}
set.seed(418910)

numEmail = length(isSpam)
numSpam = sum(isSpam)
numHam = numEmail - numSpam

# take 1/3 of data for testing from each class spam ham
testSpamIdx = sample(numSpam, size =floor(numSpam/3))
testHamIdx = sample(numHam, size=floor(numHam/3))

# get word vectors from msgWordsList
# test
testMsgWords = c((msgWordsList[isSpam])[testSpamIdx],
                 (msgWordsList[!isSpam])[testHamIdx])

# train
trainMsgWords = c((msgWordsList[isSpam])[-testSpamIdx],
                 (msgWordsList[!isSpam])[-testHamIdx])

# get labels for train and test
testIsSpam = rep(c(TRUE, FALSE), 
                 c(length(testSpamIdx), length(testHamIdx)))

trainIsSpam = rep(c(TRUE, FALSE), 
                 c(numSpam - length(testSpamIdx), 
                   numHam - length(testHamIdx)))

```

```{r nbayes_freq_table, echo=FALSE, include=FALSE, eval=FALSE}

computeFreqs <- function(wordsList, spam, bow = unique(unlist(wordsList))) {
   # create a matrix for spam, ham, and log odds
  wordTable = matrix(0.5, nrow = 4, ncol = length(bow), # default value is 0.5
                     dimnames = list(c("spam", "ham",  # name dimensions
                                        "presentLogOdds", 
                                        "absentLogOdds"),  bow))

  # build frequency table 
  # For each spam message, add 1 to counts for words in message
  counts.spam = table(unlist(lapply(wordsList[spam], unique)))
  wordTable["spam", names(counts.spam)] = counts.spam + .5

   # Similarly for ham messages
  counts.ham = table(unlist(lapply(wordsList[!spam], unique)))  
  wordTable["ham", names(counts.ham)] = counts.ham + .5  


   # Find the total number of spam and ham
  numSpam = sum(spam) # from T/F vector passed to func
  numHam = length(spam) - numSpam

  # Prob(word|spam) and Prob(word | ham)
  # compute probabilities for each word given spam or ham
  wordTable["spam", ] = wordTable["spam", ]/(numSpam + .5)
  wordTable["ham", ] = wordTable["ham", ]/(numHam + .5)
  
   # calculate log odds
  # log odds of presence
  wordTable["presentLogOdds", ] = 
     log(wordTable["spam",]) - log(wordTable["ham", ])
  # log odds of absence
  wordTable["absentLogOdds", ] = 
     log((1 - wordTable["spam", ])) - log((1 -wordTable["ham", ]))

  invisible(wordTable)
}

```

```{r nbayes_classify, echo=FALSE, include=FALSE, eval=FALSE}

# use our frequency and odds table to classify new messages
computeMsgLLR = function(words, freqTable) 
{
       # Discards words not in training data.
  words = words[!is.na(match(words, colnames(freqTable)))]

       # Find which words are present
  present = colnames(freqTable) %in% words

  sum(freqTable["presentLogOdds", present]) +
    sum(freqTable["absentLogOdds", !present])
}

testLLR = sapply(testMsgWords, computeMsgLLR, trainTable)

tapply(testLLR, testIsSpam, summary)

pdf("SP_Boxplot.pdf", width = 6, height = 6)
spamLab = c("ham", "spam")[1 + testIsSpam]
boxplot(testLLR ~ spamLab, ylab = "Log Likelihood Ratio",
      #  main = "Log Likelihood Ratio for Randomly Chosen Test Messages",
        ylim=c(-500, 500))
dev.off()

typeIErrorRate = 
function(tau, llrVals, spam)
{
  classify = llrVals > tau
  sum(classify & !spam)/sum(!spam)
}

typeIErrorRate(0, testLLR,testIsSpam)

typeIErrorRate(-20, testLLR,testIsSpam)

typeIErrorRates = 
function(llrVals, isSpam) 
{
  o = order(llrVals)
  llrVals =  llrVals[o]
  isSpam = isSpam[o]

  idx = which(!isSpam)
  N = length(idx)
  list(error = (N:1)/N, values = llrVals[idx])
}

typeIIErrorRates = function(llrVals, isSpam) {
    
  o = order(llrVals)
  llrVals =  llrVals[o]
  isSpam = isSpam[o]
    
    
  idx = which(isSpam)
  N = length(idx)
  list(error = (1:(N))/N, values = llrVals[idx])
  }  

xI = typeIErrorRates(testLLR, testIsSpam)
xII = typeIIErrorRates(testLLR, testIsSpam)
tau01 = round(min(xI$values[xI$error <= 0.01]))
t2 = max(xII$error[ xII$values < tau01 ])

pdf("LinePlotTypeI+IIErrors.pdf", width = 8, height = 6)

library(RColorBrewer)
cols = brewer.pal(9, "Set1")[c(3, 4, 5)]
plot(xII$error ~ xII$values,  type = "l", col = cols[1], lwd = 3,
     xlim = c(-300, 250), ylim = c(0, 1),
     xlab = "Log Likelihood Ratio Values", ylab="Error Rate")
points(xI$error ~ xI$values, type = "l", col = cols[2], lwd = 3)
legend(x = 50, y = 0.4, fill = c(cols[2], cols[1]),
       legend = c("Classify Ham as Spam", 
                  "Classify Spam as Ham"), cex = 0.8,
       bty = "n")
abline(h=0.01, col ="grey", lwd = 3, lty = 2)
text(-250, 0.05, pos = 4, "Type I Error = 0.01", col = cols[2])

mtext(tau01, side = 1, line = 0.5, at = tau01, col = cols[3])
segments(x0 = tau01, y0 = -.50, x1 = tau01, y1 = t2, 
         lwd = 2, col = "grey")
text(tau01 + 20, 0.05, pos = 4,
     paste("Type II Error = ", round(t2, digits = 2)), 
     col = cols[1])

dev.off()

k = 5
numTrain = length(trainMsgWords)
partK = sample(numTrain)
tot = k * floor(numTrain/k)
partK = matrix(partK[1:tot], ncol = k)

testFoldOdds = NULL
for (i in 1:k) {
  foldIdx = partK[ , i]
  trainTabFold = computeFreqs(trainMsgWords[-foldIdx], trainIsSpam[-foldIdx])
  testFoldOdds = c(testFoldOdds, 
               sapply(trainMsgWords[ foldIdx ], computeMsgLLR, trainTabFold))
}

testFoldSpam = NULL
for (i in 1:k) {
  foldIdx = partK[ , i]
  testFoldSpam = c(testFoldSpam, trainIsSpam[foldIdx])
}

xFoldI = typeIErrorRates(testFoldOdds, testFoldSpam)
xFoldII = typeIIErrorRates(testFoldOdds, testFoldSpam)
tauFoldI = round(min(xFoldI$values[xFoldI$error <= 0.01]))
tFold2 = xFoldII$error[ xFoldII$values < tauFoldI ]

```
