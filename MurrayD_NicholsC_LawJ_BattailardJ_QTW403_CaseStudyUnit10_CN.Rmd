---
title: "Spam Detection: Update this title"
author: "Dennis Murray, Jared Law, Julien Bataillard, Cory Nichols"
date: "March 20th, 2018"
output:
  word_document:
    fig_caption: yes
section: MSDS 7333-403 - Quantifying the World - Case Study 5 (Unit 10)
---

## Abstract

## Introduction

## Literature Review

## Methods


#### rpart Model Comparison

We explore the effect of rpart's control parameters on classifying spam emails by fitting a default rpart model and comparing it to a separate, optimized rpart model. Specifically, we explore four different parameters: complexity penalty (cp), minsplit, maxdepth, and the splitting criteria. Table XX provides a description for each of these parameters. 

Table XX
cp  - A scaled complexity penalty that ranges from 0 to 1. In a classification setting, cp is compared against the error rate relative to a previous split. Any split that does not decrease the overall lack of fit by cp is not considered.

minsplit - the minimum number of observations that must exist in a node in order for a split to be attempted.

maxdepth - The maximum depth of any node of the final tree, with the root node counted as depth 0. 

splitting criteria - gini or information. Gini utilizes the gini index to optimize split points, information uses entropy and information gain.

Utilizing the full listing of variables in our email dataset can lead to overfitting, however, decision trees allow us to find the best variables for splitting while pruning extraneous splits. Applying the first three parameters previously mentioned typically reduces the size of the final tree. This reduction in variance should help with model generalization to the test dataset. 

While rpart contains other control parameters, they are used primarily for exploratory purposes (xx reference). However, one point of clarification regarding the parameter xval is warranted. The parameter xval allows a user to optimize the cost penalty (cp) for a tree in a k-fold cross-validation setting. Given rpart does not allow for the tuning of multiple parameters simultaneously, we instead rely on the mlr package (xx reference) for cross-validation and tuning. 

We use 80 percent of the email data for training and 20 percent for testing. Spam represents roughly 25 percent of the emails in our original dataset. Therefore, we stratify the observations in our train and test datasets to maintain the original spam/ham ratio. For example, of the 20 percent of data held for testing, 25 percent is made up of spam. We do not explore weighting over oversampling procedures in our analysis. 

Additionally, we seek to maximize the true positive classification rate where "spam" is our positive class. We do this by selecting area under the ROC curve (AUC) as our performance metric. A false positive means an important email may end up in spam or deleted. A false negative means the user may experience unfiltered messages that should be in the spam folder. We consider the former situation a more severe model error.

In order to explore the effect of rpart parameters mentioned previously, we first fit an rpart model using all 29 features and default model parameters. Default parameters for rpart consist of a minsplit of 20, a complexity paramer (cp) of 0.01, maxdepth of 30 and the gini index for splitting purposes.

isSpam ~ ., ma

We retain the initial fit on the full training set in order to evaluate the model's generalization capabilities on the test data set. We call this model our "base" model, and use this terminology going forward.

For comparison purposes, an optimized model is also fit on the training data set. Optimization is achieved by exploring a discrete list of the four parameters of interest. A grid search method is used with ten-fold stratified cross-validation. Each combination of parameters in Table XX below is tested via cross-validation. The mean AUCs for all model and associated parameters are compared and the model with the highest cross-validated AUC is chosen.

table list of discrete vars here

The base rpart model with default parameters is then subsequently compared to the optimized model given the test data set.


## Results

### Base Model Results
show model var importance here

show top 2 classification bounds viz


### Optimized Model Results

show hyperparam effects viz (iter and heatmaps)


### Model Comparisons

conclude
show performance results
show ROC curve

## Conclusions and Future Work

## References

https://cran.r-project.org/web/packages/rpart/rpart.pdf
https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf
https://stats.stackexchange.com/questions/191842/how-does-the-complexity-parameter-correspond-to-the-number-of-splits-in-cross-va


```{r setup, include=FALSE, cache=TRUE}
dir <- "~/DataScience/SMU/QTW/Unit10/CaseStudy/"
setwd(dir)
knitr::opts_knit$set(root.dir = dir)
knitr::opts_chunk$set(echo = FALSE)
```

```{r load_libs, include=FALSE, cache=TRUE}
library(magrittr)
library(dplyr)
library(plyr)
library(tidyr)
library(ggplot2)
library(tm)
library(rpart)
library(caret)
library(mlr)
library(parallelMap)
library(doParallel)
library(missForest)
```

```{r make, include=FALSE, cache=TRUE, eval=FALSE}
# get and clean data
source("src/make.R")
```

```{r eda, include=FALSE}

# basic EDA stuff
isSpamLabels <- emailDF$isSpam

boxplot(log(1+emailDF$perCaps) ~emailDF$isSpam)
qqplot((1+log(emailDF[emailDF$isSpam==TRUE,]$perCaps)), (1+log(emailDF[emailDF$isSpam==FALSE,]$perCaps)))
table(emailDF$numAtt, emailDF$isSpam)

# spam typically has no reply.
colM = c("#E41A1C80", "#377EB880")
isRe = factor(emailDF$isRe, labels=c("no Re:", "Re:"))
mosaicplot(table(isSpamLabels, isRe), main ="", xlab="", ylab="", color=colM)

# from email ending in numbers, ham usually has no ending #
fromNE = factor(emailDF$numEnd, labels=c("No #", "#"))
mosaicplot(table(isSpamLabels, fromNE), color=colM, main="", xlab="", ylab="")

# insert correlation matrix here for numerics?
# ANOVAs for comparing categorical and numeric?
# chi squared tests for categorical features independence from one another?
# LVQ for selecting optimal features in general?

```

```{r fit_dtree, include=FALSE}
# set seed for reproducibility
set.seed(418910) 

# rename and relevel vars for better interpretation
emailDFrp$isSpam <- emailDFrp$isSpam %>% 
                      revalue(c("T"="Spam", "F"="Ham")) %>% 
                        relevel("Spam")

# get counts to prep for train/test split
spam <- emailDFrp$isSpam == "Spam"
numSpam <- sum(spam)
numHam <- sum(!spam)

# 80/20 split, stratified
testSpamIdx <- sample(numSpam, size = floor(numSpam/5))
testHamIdx <- sample(numHam, size = floor(numHam/5))

# impute missing vals based on forest classification 
# and regression
registerDoParallel(cores=4)
df <- missForest(emailDFrp, 
                 maxiter=5, 
                 ntree=100, 
                 parallelize = c('forests'),
                 variablewise = TRUE)

emailDFrp <- df$ximp

# pull together stratified train and test sets with training 80 pct
testDF <- 
  rbind( emailDFrp[emailDFrp$isSpam == "Spam", ][testSpamIdx, ],
         emailDFrp[emailDFrp$isSpam == "Ham", ][testHamIdx, ])

trainDF <-
  rbind( emailDFrp[emailDFrp$isSpam == "Spam", ][-testSpamIdx, ], 
         emailDFrp[emailDFrp$isSpam == "Ham", ][-testHamIdx, ])

# https://mlr-org.github.io/mlr-tutorial/release/html/task/index.html

# initiate mlr classification task
# set up a learning task placeholder
spam.tsk = makeClassifTask(id = "spam", 
                           data = trainDF, 
                           target = "isSpam")

print(spam.tsk)
getTaskNFeats(spam.tsk)
getTaskClassLevels(spam.tsk)

# Create the learner from embedded libraries
spam.lrn = makeLearner( cl = "classif.rpart",# use rpart algorithm, gini index is default for splitting
                        id ="spam", # give it an id
                        fix.factors.prediction = TRUE, # control for missing class if any)
                        predict.type = 'prob') # to get probabilities
print(spam.lrn)

# focus on maxdepth, cp and minsplit
# show defaults - cp = 0.01, minsplit=20, maxdepth=30
getParamSet(spam.lrn) 

# what types of predictions
spam.lrn$predict.type
spam.lrn$par.set

# fit with defaults, cp = 0.01
spam.clf <- mlr::train(spam.lrn, spam.tsk)
splits <- getLearnerModel(spam.clf)
# check out CP, default gini index
summary(splits)

# which variables are most important?
dat <- data.frame(vars=names(splits$variable.importance), 
                  importance=splits$variable.importance)

ggplot(dat, aes(reorder(vars, importance, sum), importance))+
    coord_flip()+
    geom_col()

#barchart(sort(splits$variable.importance))

spam.pred <- predict(spam.clf, newdata =  testDF)
listMeasures(spam.tsk)
performance(spam.pred, measures = list(auc, fpr, fnr, mmce))

# get scores
preds <- as.data.frame(spam.pred$data)
getPredictionProbabilities(spam.pred) # returns on prob for spam
# getPredictionResponse(spam.pred)
# getPredictionTruth()

# confusion matrix for default rpart
calculateConfusionMatrix(spam.pred)
calculateConfusionMatrix(spam.pred, relative = TRUE)


# for viz purposes, log top 2 importance, fit and predict
trainDF$perCaps <- log(1+trainDF$perCaps)
trainDF$bodyCharCt <- log(1+trainDF$bodyCharCt)

# obviously a lot of error when just using the top two predictors
# needs more to be accurate!
spam.log.tsk = makeClassifTask(id = "spam", 
                               data = trainDF, 
                               target = "isSpam")

g <- plotLearnerPrediction(learner = spam.lrn, 
                           task = spam.log.tsk, 
                           features=c("perCaps", "bodyCharCt"),
                           pointsize = 0.5, 
                           err.col="white",
                           bg.cols = c("darkblue","green"),
                           err.size = 0.5,
                           err.mark="cv")

g+
  ggtitle('Figure XX: Body Character Count and Percent Capitals')

# what does our optimal threshold look like for our metrics?
n = getTaskSize(spam.tsk)
thresh_plot = generateThreshVsPerfData(spam.pred, measures = list(auc, mmce))
plotThreshVsPerf(thresh_plot)


# RESAMPLING
spam.resamp = makeResampleDesc(method = "CV", 
                               iters = 10, 
                               stratify = TRUE)

# pull out resampling instance for reproducibility
#spam.resamp.bin = makeResampleInstance(spam.resamp, task=spam.tsk)
#save(spam.resamp.bin, file ="resampling_binary") # save for future use!
load("resampling_binary")


# TUNING 
# identify search grid params - we'll brute force here based on reasonable vals
# can also random search over numeric ranges instead
# test individual params (if range makeNumericParam(param, lower=, upper=))
spam.ps = makeParamSet(
                  makeDiscreteParam("cp", values = c(0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.5)),
                  makeDiscreteParam("minsplit", values = c(1, 5, 10, 15, 20, 30, 40)),
                  makeDiscreteParam("maxdepth", values = c(2, 5, 7, 10, 15, 20, 30, 40)),
                  makeDiscreteParam("parms", values = list(gini = list(split = c("gini")), 
                                                           info = list(split = c("information"))))
                 )

print(spam.ps)
# Create the grid and identify algorithm

# set up control, no params passed as we are searching
# over a discrete set of values
spam.ctrl = makeTuneControlGrid() 

# launch parallel multicore for tuning
parallelStart(mode="multicore", cpus = 4)

# tune parameters for our learner, task, optimize for auc
spam.tuned.clf = tuneParams( learner = spam.lrn, 
                             task = spam.tsk, 
                             resampling = spam.resamp.bin, # compare to same resampling method!
                             control = spam.ctrl, 
                             par.set = spam.ps, 
                             measures = list(auc))
parallelStop()

# hyperparam effects
data <- generateHyperParsEffectData(spam.tuned.clf, 
                                    partial.dep = TRUE)

# partial dependency plots based on random forest regression
plotHyperParsEffect(data, x = "iteration", y = "auc.test.mean",
  plot.type = "line", partial.dep.learn = "regr.randomForest")

plotHyperParsEffect(data, x = "cp", y = "maxdepth", z = "auc.test.mean",
  plot.type = "heatmap", partial.dep.learn = "regr.randomForest")

plotHyperParsEffect(data, x = "minsplit", y = "maxdepth", z = "auc.test.mean",
  plot.type = "heatmap", partial.dep.learn = "regr.randomForest")


# get optimal hyperparameters for use on test
spam.opt.lrn <- setHyperPars(spam.lrn, par.vals = spam.tuned.clf$x)
spam.opt.clf <- mlr::train(spam.opt.lrn, spam.tsk)

# results
spam.opt.preds <- predict(spam.opt.clf, newdata = testDF)
spam.preds <- predict(spam.clf, newdata =  testDF)

performance(spam.preds, measures=list(auc, mmce))
performance(spam.opt.preds, measures=list(auc, mmce))


df = generateThreshVsPerfData(list(base = spam.preds, 
                                   optimized = spam.opt.preds), 
                                   measures = list(fpr, tpr))

# whooped that a**
plotROCCurves(df)
qplot(x = fpr, y = tpr, color = learner, data = df$data, geom = "path")

```

```{r dtree_explore, include=FALSE}
# this block is exploration of the mlr package in R
# optimize both fnr and fpr
spam.ctrl = makeTuneMultiCritControlGrid()

library(emoa)
spam.tuned.clf = tuneParamsMultiCrit( learner = spam.lrn, 
                                      task = spam.tsk, 
                                      resampling = spam.resamp.bin, # compare to same resampling method!
                                      control = spam.ctrl, 
                                      par.set = spam.ps, 
                                      measures = list(fnr, fpr))

df <- as.data.frame(trafoOptPath(spam.tuned.clf$opt.path))
df[df$fnr.test.mean+df$fpr.test.mean == min(df$fnr.test.mean+df$fpr.test.mean),] # use this to fit
lrn = setHyperPars(makeLearner("classif.rpart"), par.vals = list(cp=0.001, minsplit=5, maxdepth=15))


# get a list of preds for each fold
getRRPredictionList(spam.base.clf)
# get all ze data!
pred <- getRRPredictions(spam.base.clf)
head(pred$data)

# get individual model fits, turn on models = TRUE in resample()
spam.base.clf$models

# establish train and test (learning curve here?)
# 80/20 split works best
spam.tsk = makeClassifTask(id = "spam", 
                           data = df$ximp, 
                           target = "isSpam", 
                           check.data = FALSE)

spam.lrn = makeLearner("classif.rpart",  # use rpart algorithm, gini index is default for splitting
                        id ="spam", # give it an id
                        fix.factors.prediction = TRUE, # control for missing class if any)
                        predict.type = 'prob') 

r <- generateLearningCurveData(
  learners = spam.opt.lrn,
  task = spam.tsk,
  percs = seq(0.1, 1, by = 0.1),
  measures = list(auc, setAggregation(auc, train.mean)),
  resampling = makeResampleDesc(method = "CV", iters = 10, stratify = TRUE, predict="both"),
  show.info = FALSE)

plotLearningCurve(r, facet="learner")

# try some nested resampling for unbiased error assessment
# on an outer CV, use inner to tune params, find best
# train model on outer CV splits based on these params and test on outer CV split
parallelStart(mode="multicore", cpus = 4)

tune.lrn = makeTuneWrapper(learner = spam.lrn,
                           resampling = spam.resamp, # inner resampling loop
                           par.set = spam.ps, # parameters to test
                           control = spam.ctrl, # control
                           measures = auc,
                           show.info = FALSE)

# unbiased test outer cross validation
outer = makeResampleDesc("CV", iters = 3, stratify = TRUE)

r = resample(tune.lrn, 
             spam.tsk, 
             resampling = outer, 
             extract = getTuneResult,
             measures = auc,
             show.info = FALSE)

parallelStop()

getNestedTuneResultsX(r)
data <- generateHyperParsEffectData(r, partial.dep = TRUE)
plotHyperParsEffect(data[[1]], x = "iteration", y = "auc.test.mean",
                    plot.type = "line", partial.dep.learn = "regr.randomForest")


```

```{r nbayes_get_data, include=FALSE}
# RSpamData not available in R 3.4.3

spamPath <- "Data"

dirNames <- list.files(path = spamPath, full.names = TRUE)

fileNames <- list.files(dirNames[1], full.names = TRUE)

sapply(dirNames, function(x) length(list.files(x)))

# pick random emails
idx = c(1:5, 15, 27, 68, 69, 329, 404, 427, 516, 852, 971)

#sampleEmail = sapply(list.files(dirNames[1], full.names = TRUE)[idx], readLines)

# split into header and body
splitMessage <- function(msg) {
  splitPoint = match("", msg) # find first blank line
  header = msg[1:(splitPoint-1)] # get header, will use this for attachments
  body = msg[ -(1:splitPoint) ] # get body
  return(list(header = header, body = body))
}

sampleSplit <- lapply(sampleEmail, splitMessage)

# get headers
headerList = lapply(sampleSplit, function(msg) msg$header)

# get boundary id from multi-part email

getBoundary<- function(header) {
  boundaryIdx = grep("boundary=", header) # index boundary
  boundary = gsub('"', "", header[boundaryIdx]) # replace quotes
  gsub(".*boundary= *([^;]*);?.*", "\\1", boundary) # regexrepl with first group match
}

# pass boundary and body of each message
dropAttach = function(body, boundary){
  bString = paste("--", boundary, sep = "") 
  bStringLocs = which(bString == body)# find start locs for each section of body
  # -- is arbitrary
  
  if (length(bStringLocs) <= 1) return(body) # if no boundaries, give me back body
  
  eString = paste("--", boundary, "--", sep = "") # ending boundary string
  eStringLoc = which(eString == body) # ending string loc
  if (length(eStringLoc) == 0)  # if no ending string return first part of body
    return(body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1)])
  
  # otherwise get length of body
  n = length(body)
  # if ending boundary string < rows, return body + ending rows after ending boundary
  if (eStringLoc < n) 
     return( body[ c( (bStringLocs[1] + 1) : (bStringLocs[2] - 1), 
                    ( (eStringLoc + 1) : n )) ] )
  
  # finally, return body if no other conditions met
  return( body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1) ])
}

# clean up punctuation, numbers, spaces
cleanText =
function(msg)   {
  tolower(gsub("[[:punct:]0-9[:space:][:blank:]]+", " ", msg))
}

findMsgWords = 
function(msg, stopWords) {
 if(is.null(msg))
  return(character())

  # split and return vector of cleaned words
 words = unique(unlist(strsplit(cleanText(msg), "[[:blank:]\t]+")))
 
 # drop empty and 1 letter words
 words = words[ nchar(words) > 1]
 words = words[ !( words %in% stopWords) ]
 invisible(words)
}

processAllWords = function(dirName, stopWords)
{
  # read all files in a given directory
  fileNames = list.files(dirName, full.names = TRUE)
  # drop files that are not email
  notEmail = grep("cmds$", fileNames)
  
  if ( length(notEmail) > 0) fileNames = fileNames[ - notEmail ]

  # read all messages into a list
  messages = lapply(fileNames, readLines, encoding = "latin1")
  
  # split header and body for each message
  emailSplit = lapply(messages, splitMessage)
  # put body and header in own lists
  bodyList = lapply(emailSplit, function(msg) msg$body)
  headerList = lapply(emailSplit, function(msg) msg$header)
  # save some memory, kill emailSplit var
  rm(emailSplit)
  
  # determine which messages have attachments
  hasAttach = sapply(headerList, function(header) {
    CTloc = grep("Content-Type", header)
    if (length(CTloc) == 0) return(0)
    multi = grep("multi", tolower(header[CTloc])) 
    if (length(multi) == 0) return(0)
    multi # has attachment
  })
  
  # get indices for which messages have attachments
  hasAttach = which(hasAttach > 0)
  
  # find boundary strings for messages with attachments
  boundaries = sapply(headerList[hasAttach], getBoundary)
  
  # drop attachments from message bodies
  bodyList[hasAttach] = mapply(dropAttach, bodyList[hasAttach], 
                               boundaries, SIMPLIFY = FALSE)
  
  # extract words from body
  msgWordsList = lapply(bodyList, findMsgWords, stopWords)
  
  invisible(msgWordsList)
}

msgWordsList = lapply(dirNames, processAllWords, stopWords=stopWords)

numMsgs = sapply(msgWordsList, length)

# identify which messages are spam based on num Msgs
isSpam = rep(c(FALSE, FALSE, FALSE, TRUE, TRUE), numMsgs)

# list of vectors of cleaned words for each message
msgWordsList = unlist(msgWordsList, recursive = FALSE)
```

``` {r nbayes_train_test_split, include=FALSE}
set.seed(418910)

numEmail = length(isSpam)
numSpam = sum(isSpam)
numHam = numEmail - numSpam

# take 1/3 of data for testing from each class spam ham
testSpamIdx = sample(numSpam, size =floor(numSpam/3))
testHamIdx = sample(numHam, size=floor(numHam/3))

# get word vectors from msgWordsList
# test
testMsgWords = c((msgWordsList[isSpam])[testSpamIdx],
                 (msgWordsList[!isSpam])[testHamIdx])

# train
trainMsgWords = c((msgWordsList[isSpam])[-testSpamIdx],
                 (msgWordsList[!isSpam])[-testHamIdx])

# get labels for train and test
testIsSpam = rep(c(TRUE, FALSE), 
                 c(length(testSpamIdx), length(testHamIdx)))

trainIsSpam = rep(c(TRUE, FALSE), 
                 c(numSpam - length(testSpamIdx), 
                   numHam - length(testHamIdx)))

```

```{r nbayes_freq_table, include=FALSE}

computeFreqs <- function(wordsList, spam, bow = unique(unlist(wordsList))) {
   # create a matrix for spam, ham, and log odds
  wordTable = matrix(0.5, nrow = 4, ncol = length(bow), # default value is 0.5
                     dimnames = list(c("spam", "ham",  # name dimensions
                                        "presentLogOdds", 
                                        "absentLogOdds"),  bow))

  # build frequency table 
  # For each spam message, add 1 to counts for words in message
  counts.spam = table(unlist(lapply(wordsList[spam], unique)))
  wordTable["spam", names(counts.spam)] = counts.spam + .5

   # Similarly for ham messages
  counts.ham = table(unlist(lapply(wordsList[!spam], unique)))  
  wordTable["ham", names(counts.ham)] = counts.ham + .5  


   # Find the total number of spam and ham
  numSpam = sum(spam) # from T/F vector passed to func
  numHam = length(spam) - numSpam

  # Prob(word|spam) and Prob(word | ham)
  # compute probabilities for each word given spam or ham
  wordTable["spam", ] = wordTable["spam", ]/(numSpam + .5)
  wordTable["ham", ] = wordTable["ham", ]/(numHam + .5)
  
   # calculate log odds
  # log odds of presence
  wordTable["presentLogOdds", ] = 
     log(wordTable["spam",]) - log(wordTable["ham", ])
  # log odds of absence
  wordTable["absentLogOdds", ] = 
     log((1 - wordTable["spam", ])) - log((1 -wordTable["ham", ]))

  invisible(wordTable)
}

```

```{r nbayes_classify, include=FALSE}

# use our frequency and odds table to classify new messages
computeMsgLLR = function(words, freqTable) 
{
       # Discards words not in training data.
  words = words[!is.na(match(words, colnames(freqTable)))]

       # Find which words are present
  present = colnames(freqTable) %in% words

  sum(freqTable["presentLogOdds", present]) +
    sum(freqTable["absentLogOdds", !present])
}

testLLR = sapply(testMsgWords, computeMsgLLR, trainTable)

tapply(testLLR, testIsSpam, summary)

pdf("SP_Boxplot.pdf", width = 6, height = 6)
spamLab = c("ham", "spam")[1 + testIsSpam]
boxplot(testLLR ~ spamLab, ylab = "Log Likelihood Ratio",
      #  main = "Log Likelihood Ratio for Randomly Chosen Test Messages",
        ylim=c(-500, 500))
dev.off()

typeIErrorRate = 
function(tau, llrVals, spam)
{
  classify = llrVals > tau
  sum(classify & !spam)/sum(!spam)
}

typeIErrorRate(0, testLLR,testIsSpam)

typeIErrorRate(-20, testLLR,testIsSpam)

typeIErrorRates = 
function(llrVals, isSpam) 
{
  o = order(llrVals)
  llrVals =  llrVals[o]
  isSpam = isSpam[o]

  idx = which(!isSpam)
  N = length(idx)
  list(error = (N:1)/N, values = llrVals[idx])
}

typeIIErrorRates = function(llrVals, isSpam) {
    
  o = order(llrVals)
  llrVals =  llrVals[o]
  isSpam = isSpam[o]
    
    
  idx = which(isSpam)
  N = length(idx)
  list(error = (1:(N))/N, values = llrVals[idx])
  }  

xI = typeIErrorRates(testLLR, testIsSpam)
xII = typeIIErrorRates(testLLR, testIsSpam)
tau01 = round(min(xI$values[xI$error <= 0.01]))
t2 = max(xII$error[ xII$values < tau01 ])

pdf("LinePlotTypeI+IIErrors.pdf", width = 8, height = 6)

library(RColorBrewer)
cols = brewer.pal(9, "Set1")[c(3, 4, 5)]
plot(xII$error ~ xII$values,  type = "l", col = cols[1], lwd = 3,
     xlim = c(-300, 250), ylim = c(0, 1),
     xlab = "Log Likelihood Ratio Values", ylab="Error Rate")
points(xI$error ~ xI$values, type = "l", col = cols[2], lwd = 3)
legend(x = 50, y = 0.4, fill = c(cols[2], cols[1]),
       legend = c("Classify Ham as Spam", 
                  "Classify Spam as Ham"), cex = 0.8,
       bty = "n")
abline(h=0.01, col ="grey", lwd = 3, lty = 2)
text(-250, 0.05, pos = 4, "Type I Error = 0.01", col = cols[2])

mtext(tau01, side = 1, line = 0.5, at = tau01, col = cols[3])
segments(x0 = tau01, y0 = -.50, x1 = tau01, y1 = t2, 
         lwd = 2, col = "grey")
text(tau01 + 20, 0.05, pos = 4,
     paste("Type II Error = ", round(t2, digits = 2)), 
     col = cols[1])

dev.off()

k = 5
numTrain = length(trainMsgWords)
partK = sample(numTrain)
tot = k * floor(numTrain/k)
partK = matrix(partK[1:tot], ncol = k)

testFoldOdds = NULL
for (i in 1:k) {
  foldIdx = partK[ , i]
  trainTabFold = computeFreqs(trainMsgWords[-foldIdx], trainIsSpam[-foldIdx])
  testFoldOdds = c(testFoldOdds, 
               sapply(trainMsgWords[ foldIdx ], computeMsgLLR, trainTabFold))
}

testFoldSpam = NULL
for (i in 1:k) {
  foldIdx = partK[ , i]
  testFoldSpam = c(testFoldSpam, trainIsSpam[foldIdx])
}

xFoldI = typeIErrorRates(testFoldOdds, testFoldSpam)
xFoldII = typeIIErrorRates(testFoldOdds, testFoldSpam)
tauFoldI = round(min(xFoldI$values[xFoldI$error <= 0.01]))
tFold2 = xFoldII$error[ xFoldII$values < tauFoldI ]

```
