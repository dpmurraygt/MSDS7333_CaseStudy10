---
title: "Spam Detection: Update this title"
author: "Dennis Murray, Jared Law, Julien Bataillard, Cory Nichols"
date: "March 20th, 2018"
output:
  word_document:
    fig_caption: yes
section: MSDS 7333-403 - Quantifying the World - Case Study 5 (Unit 10)
---

```{r setup, include=FALSE, cache=TRUE}
dir <- "~/DataScience/SMU/QTW/Unit10/CaseStudy/"
setwd(dir)
knitr::opts_knit$set(root.dir = dir)
knitr::opts_chunk$set(echo = FALSE)
```

```{r load_libs, include=FALSE, cache=TRUE}
library(magrittr)
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(tm)
library(rpart)
library(caret)
library(mlr)
library(parallelMap)
library(doParallel)
library(missForest)
library(scales)
library(ggthemes)
library(GGally)
library(ltm)
```

```{r make, include=FALSE, cache=TRUE, eval=FALSE}
# get and clean data
source("src/make.R")

# set seed for reproducibility
set.seed(418910) 

# rename and relevel vars for better interpretation
emailDFrp$isSpam <- emailDFrp$isSpam %>% 
                      revalue(c("T"="Spam", "F"="Ham")) %>% 
                        relevel("Spam")

```

## Abstract

The use of email has grown exponentially since the introduction of the world wide web in the late 20th century. Today, spam email is ubiquotous on every email platform. Spam detection methods to filter out unwanted emails have originated in the late 1990s and while the algorithms have improved, so have spam avoidance methods. In this paper, we explore rpart; a classification and regression tree package in R. Specifically, we explore the effectiveness of spam classification using rpart and its parameters on a dataset of emails previously classified as spam or ham (valid email). After parameter optimization and cross validation, our rpart model outperforms a rpart model with default parameters.

## Introduction

Email is an integral part of our lives. The overall information spread through the use of email is far and wide, and includes unwanted marketing and phishing. When email was introduced in the late 1980s, it spread rapidly, and so did the ability for companies to market products to email users. In addition, entities with ulterior motives sprang up, attempting to gain information from people using phishing and social engineering. These unwanted emails were termed "spam."

Spam filters were introduced not long after the introduction of email. These filters automatically process the incoming messages, applying certain techniques to identify unwanted emails.  Bayesian email filters began to be utilized in 1996 but didnâ€™t become popular until much later. These techniques utilize the probabilities of certain words occurring in regular emails versus spam emails.

Today, many different methods exist for spam email classification. We explore a decision tree package in R called rpart. Our objective is to investigate and optimize key hyperparameters used in rpart in order to classify email messages as spam or not spam. Specifically, we analyze two separate decision trees for classifying spam email using the rpart package. We fit one decision tree on training data using default hyperparameters in rpart. After optimizing the minsplit, maxdepth and complexity parameters, we fit a second decision tree on the training data. We compare each model's generalization performance on test email data. Our metric of choice is AUC or the area under the ROC curve. We use this metric to maximize the true positive rate of our classifiers.

In the subsquent section, we review research literature to catalogue spam filtering techniques in academics and business. As part of the methods section, we explore a dataset of emails tagged as spam or not spam. We then implement two different decision trees and compare their generalization performance to determine if hyperparameter tuning results in better performance. Our paper concludes with a discussion of the applications of the improvements in anti-spam email filtering.

## Literature Review

## Methods

Prior to exploring and fitting a decision tree using rpart, we briefly introduce and explore the email dataset. Specifically, predictor variable relationships are examined using correlation and indepdence methods. Relationships of predictor variables to the response variable are also investigated visually. We also introduce variable importance, an rpart method that will allow us to efficiently determine optimal variables for splitting.

### Exploratory Data Analysis

The email dataset is made up of a corpus of emails from SpamAssassin.org (xx). In total, there are 9348 unique emails. Each observation is made up of 29 predictor variables and one response variable. Of the 30 total variables, 17 are boolean factor variables and the remaining 13 variables represent numeric variables. Our task is to classify isSpam, a boolean variable with levels of "Spam" and "Ham."

Numeric vars - 
Boolean vars - 

For context, a sample description of five variables is given below:

*perCaps: percentage of capitals in the email body
*isYelling: subject alpha characters are all capital
*bodyCharCt: number of characters in the body of the email
*numEnd: email ends in numbers
*isRe: subject contains reply characters Re:

A preliminary evaluation of the data found missing data in 303 unique rows.  The availability of values for the following predictor variables was incomplete for:
* subSpamWords
* subQuesCt
* subExcCt
* subBlanks
* numRec
* noHost
* isYelling

Rather than discard over two percent of the dataset, we undertake imputation using random forest imputation methods for both numeric and categorical predictor variables.

```{r dataset_bal, include=TRUE, fig.height=3.5, fig.width=8.5}
emailDFrp %>% 
  group_by(isSpam) %>% 
    dplyr::summarise(Count=n()) %>% 
      mutate(Pct = Count/sum(Count)) %>%
      ggplot(aes(x=isSpam, y=Count, label=comma(Count))) + 
      geom_bar(stat='identity') +  theme_light() + 
      ggtitle("Figure X: Spam vs. Non-Spam Split") + 
      scale_y_continuous("Count", labels =comma, limits=c(0,7500)) + 
      scale_x_discrete("Is Spam - True or False")  + 
      geom_text(vjust=-0.5)
```

Our dataset is unbalanced, with 2,371 (26%) spam emails, and 6,674 (74%) valid emails (ham).  This imbalance in the dataset could introduce higher false negative ratesfor our analysis task.

#### Explanatory Variable Relationships

```{r impute_df, cache=TRUE, include=FALSE}


# impute missing vals based on forest classification and regression
# set up parallel method
registerDoParallel(cores=4)
# impute
df <- missForest(emailDFrp, 
                 maxiter=5, 
                 ntree=50, 
                 parallelize = c('forests'),
                 variablewise = TRUE)
# establish imputed set
emailDFrp <- df$ximp


```

```{r cor_mat, cache=TRUE, fig.height=8, fig.width=8}

# build correlation matrix
AsVector <- emailDFrp[, c(2:30)]

nums <- sapply(AsVector, is.numeric)
bools <- sapply(AsVector, is.factor)

# correlation matrix for numerical features
cormat<- (round(cor(AsVector[, nums]), 2))
cormat <- reshape::melt(cormat, na.rm=TRUE)

cormat %>% ggplot(aes(x=X1, y=X2, fill=value)) + 
           geom_tile() + 
           theme(legend.position = "bottom", 
                 axis.text.x = element_text(angle=90, 
                                            vjust=-.5)) + 
           scale_x_discrete("") + 
           scale_y_discrete("") + 
           ggtitle("Figure X: Correlation Between Numeric Predictor Variable Pairs")

```

Working with our imputed dataset, the correlation matrix in figure XX reveals several significant correlations between numerical predictors:

* numLines and bodyCharCt
* perHTML and bodyChartCt
* numDlr and SubQuestCt

The higher correlation between predictor variables could cause these variables to be collectively overweighted in modeling, as they may not be fully independent from one another. Indeed, bodyChartCt and numLines, the number of distinct lines in the body, are nearly similar variables. However, given we are using recursive partioning with rpart, collinearity issues are usually rectified. The splitting algorithm is a greedy algorithm and will select the most important similar variable. We explore this further in the results section.

Given our dataset contains 16 boolean predictor variables as factors, we display a shortened chi-square matrix in Figure XX. This figure compares factor variables only, as correlation is not an appropriate metric for analyzing nominal relationships.

```{r chi_sq_cats, include=FALSE, cache=TRUE, fig.height=8, fig.width=8}

# chi-square matrix for categorical features

Dat <- AsVector[, bools]

combos <- combn(ncol(Dat), 2)

chisqres <- adply(combos, 2, function(x) {
  test <- chisq.test(Dat[, x[1]], Dat[, x[2]])

  out <- data.frame("Row" = colnames(Dat)[x[1]]
                    , "Column" = colnames(Dat[x[2]])
                    , "Chi.Square" = round(test$statistic,3)
                    ,  "df"= test$parameter
                    ,  "p.value" = round(test$p.value, 3)
                    )
  return(out)

})  

chisqres %>% 
  ggplot(aes(x=Row, y=Column, fill=p.value)) + 
  geom_tile() + 
  theme(legend.position = "bottom", 
        axis.text.x = element_text(angle=90, vjust=0.5)) +
        scale_y_discrete("") + scale_x_discrete("Predictor Variable") + 
        ggtitle("Figure X: Chi Square Independence between Boolean Predictor Variables")

```

Obviously, significant dependence between variables exists. This makes logical sense for variables such as isWrote, which indicates if an email is electronically scribed. Since the majority of emails are electronically scribed, we can assume this variable is not as important for predicting spam or ham. Other interesting relationships in Figure X, such as the independence between priority and noHost indicate that these variables may be useful separately for analysis. We visually inspect both continuous and factor variable relationships with isSpam in the next section.

#### Response Variable Relationships

As described earlier, we know that the majority of observations in the overall dataset are classified as spam. However, we can visualize spam and ham separation for factor and continuous predictors using different techniques. 

We inspect five factor variables visually to determine viability for spam prediction: isRe, isinReplyTo, and isWrote. The variable subSpamWords is a boolean that is true when a known spam word is contained in the subject. For instance, the word "viagra" is in the subject line. Nearly all occurrences of spam occur when these factors are set to false. On the other hand, we know that mostly valid emails (ham) occur when these variables' factors are set to true. 

```{r fig.width=8.5, fig.height=4}

emailDFrp[, c(1,which(bools)+1)] %>% 
  gather(Predictor, Value, 2:ncol(emailDFrp[,c(1, which(bools)+1)])) %>% 
    filter(Predictor %in% c("isRe", "numEnd", "subSpamWords", "isInReplyTo", "isWrote")) %>%
      ggplot(aes(x=isSpam)) + 
      geom_bar() + 
      facet_grid(Value~Predictor) + 
      theme_light() + 
      ggtitle("Figure XX: Boolean Predictor Variables Outcomes", 
               subtitle = "Y Axis Faceting Shows Spam or Ham for Each Predictor") +       
    scale_y_continuous("Count", labels =comma) + 
    scale_x_discrete("Is Spam - True or False")
```

There are occurences of spam emails in both boolean statuses for factor predictor variables numEnd, and subSpamWords. However, the true cases of each predictor variable have more cases of spam than not spam. 

We can easily identify separation of classes for numeric variables by looking at log values in a box plot. We inspect each numeric value in Figure XX.

```{r include = TRUE, cache = TRUE, fig.width=8.5, fig.height=4}
emailDFrp[,c(1, which(nums)+1)] %>% 
  gather(Predictor, Value, 2:ncol(emailDFrp[,c(1, which(nums)+1)])) %>% 
    #filter(Predictor %in% c("forwards",   "perCaps", "perHTML")) %>%
      ggplot(aes(x=isSpam, y=log(1+Value))) + 
      geom_boxplot() + 
      facet_wrap(~Predictor, scales = "free_y", ncol=4) + 
      theme_light() + 
      ggtitle("Figure XX: Continous Predictor Variables Outcomes") + 
      scale_x_discrete("Spam or Ham")+
      ylab("Log Value")
```

We can visually inspect continuous predictors in order to identify variable importance. The predictor variable "forwards", which entails how many times an email has been forwarded, shows a more concentrated distribution of values in the third quartile for ham messages. The predictor variable perCaps is different - with spam cases showing a wider interquartile range. We also know from the boxplot that the median perCaps value for spam messages is higher than ham messages. Also, roughly 75 percent of ham messages have a perCaps value lower than the median perCaps value for spam messages. PerHTML (percent of HTML in email body) also provides for a decent classification variable, with the majority of the log range occurring for spam predictions.

Examination of these predictor variables gives some idea of how we should expect the rpart model to determine splits. However, we can also expect that some of the lower correlation variables might be involved in the decision of spam versus not spam as well, perhaps providing some finer detailed distinctions between the two classes. We use rpart in the next section to determine which variables are the most important for splitting.

#### Variable Selection and rpart Model Comparison

We explore variable selection and the effect of rpart's control parameters on classifying spam emails by fitting a default rpart model and comparing it to a separate, optimized rpart model. Specifically, we explore four different parameters: complexity penalty (cp), minsplit, maxdepth, and the splitting criteria. Table XX provides a description for each of these parameters. 

Table XX
* cp  - A scaled complexity penalty that ranges from 0 to 1. In a classification setting, cp is compared against the error rate relative to a previous split. Any split that does not decrease the overall lack of fit by cp is not considered.
* minsplit - the minimum number of observations that must exist in a node in order for a split to be attempted.
* maxdepth - The maximum depth of any node of the final tree, with the root node counted as depth 0. 
* splitting criteria - gini or information. Gini utilizes the gini index to optimize split points, information uses entropy and information gain.

Utilizing the full listing of variables in our email dataset can lead to overfitting, however, decision trees allow us to find the best variables for splitting while pruning extraneous variable splits. Applying the first three parameters in table XX typically reduces the size of the final tree. This reduction in variance can help with model generalization to the test dataset. 

While rpart contains other control parameters, they are used primarily for exploratory purposes (xx reference). However, one point of clarification regarding the parameter xval is warranted. The parameter xval allows a user to optimize the cost penalty (cp) for a tree in a k-fold cross-validation setting. Given rpart does not allow for the tuning of multiple parameters simultaneously, we instead rely on the mlr package (xx reference) for cross-validation and tuning. 

We use 80 percent of the email data for training and 20 percent for testing. Spam represents roughly 25 percent of the emails in our original dataset. Therefore, we stratify the observations in our train and test datasets to maintain the original spam/ham ratio. For example, of the 20 percent of data held for testing, 25 percent is made up of spam. We do not explore weighting over oversampling procedures in our analysis. 

Additionally, we seek to maximize the true positive classification rate where "spam" is our positive class. We do this by selecting area under the ROC curve (AUC) as our performance metric. A false positive means an important email may end up in spam or deleted. A false negative means the user may experience unfiltered messages that should be in the spam folder. We consider the former situation a more severe model error.

In order to explore the effect of rpart parameters mentioned previously, we first fit an rpart model using all 29 features and default model parameters. Default parameters for rpart consist of a minsplit of 20, a complexity paramer (cp) of 0.01 and a maxdepth of 30. The Gini index is used as the splitting criterion by default.

We retain this initial fit on the full training set in order to evaluate the model's generalization capabilities on the test data set. We call this model our "base" model, and use this terminology going forward. However, as previously stated, we are interested in the variables rpart considers the most important for splitting. We can easily accomplish this with rpart package methods. Figure XX shows the variables our base model considers the most important for classification.

```{r fit_dtree_base, include=TRUE}


# get counts to prep for train/test split
spam <- emailDFrp$isSpam == "Spam"
numSpam <- sum(spam)
numHam <- sum(!spam)

# 80/20 split, stratified
testSpamIdx <- sample(numSpam, size = floor(numSpam/5))
testHamIdx <- sample(numHam, size = floor(numHam/5))


# pull together stratified train and test sets with training 80 pct
testDF <- 
  rbind( emailDFrp[emailDFrp$isSpam == "Spam", ][testSpamIdx, ],
         emailDFrp[emailDFrp$isSpam == "Ham", ][testHamIdx, ])

trainDF <-
  rbind( emailDFrp[emailDFrp$isSpam == "Spam", ][-testSpamIdx, ], 
         emailDFrp[emailDFrp$isSpam == "Ham", ][-testHamIdx, ])

# https://mlr-org.github.io/mlr-tutorial/release/html/task/index.html

# initiate mlr classification task
# set up a learning task placeholder
spam.tsk = makeClassifTask(id = "spam", 
                           data = trainDF, 
                           target = "isSpam")

#print(spam.tsk)
#getTaskNFeats(spam.tsk)
#getTaskClassLevels(spam.tsk)

# Create the learner from embedded libraries
spam.lrn = makeLearner( cl = "classif.rpart",# use rpart algorithm, gini index is default for splitting
                        id ="spam", # give it an id
                        fix.factors.prediction = TRUE, # control for missing class if any)
                        predict.type = 'prob') # to get probabilities
#print(spam.lrn)

# focus on maxdepth, cp and minsplit
# show defaults - cp = 0.01, minsplit=20, maxdepth=30
#getParamSet(spam.lrn) 

# what types of predictions
#spam.lrn$predict.type
#spam.lrn$par.set

# fit with defaults, cp = 0.01
spam.clf <- mlr::train(spam.lrn, spam.tsk)
splits <- getLearnerModel(spam.clf)
# check out CP, default gini index
#summary(splits)

# which variables are most important?
dat <- data.frame(vars=names(splits$variable.importance), 
                  importance=splits$variable.importance)

# plot the feature importances
ggplot(dat, aes(reorder(vars, importance, sum), importance))+
    coord_flip()+
    geom_col()+
    ggtitle("Figure XX: Feature Importance")+
    xlab("Feature Splits")
```

At the top of the variable importance list is perCaps, or the percentage of capital alpha characters in the body. Importance represents a weighted sum of improvement in split impurity. The perCaps variable overshadows all other variables from an importance standpoint. Indeed, we saw good separation of spam and not spam in the previous section for this variable. We are able to classify 77 percent of all spam messages based on a split value of 13 percent for perCaps. In this case, the model predicts non-spam when a message contains less than 13 percent capitals. Additionally, we see email from addresses ending in numeric (numEnd) and priority provide little value for spam classification. 

Now that we've explored variable importances, we tackle our objective of exploring key parameters and fitting an optimized model. For comparison purposes, an optimized rpart decision tree model is also fit on the training data set. Optimization is achieved by exploring a discrete list of the four parameters of interest. A grid search method is used with ten-fold stratified cross-validation. Each combination of parameters below is tested via cross-validation. The mean AUCs for all models and associated parameters are compared and the model with the highest cross-validated AUC is chosen as our optimized model.

Parameter Search Criteria
*complexity parameter (cp) - 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.5,
*minsplit - 1, 5, 10, 15, 20, 30, 40,
*maxdepth - 2, 5, 7, 10, 15, 20, 30, 40
*splitting criterion - gini, information

The base rpart model with default parameters is then subsequently compared to the optimized model given the test data set. We explore model performance and comparisons in the next section.

## Results

### Base Model Results
Given our base model variable imp

show top 2 classification bounds viz
confusion matrix


### Optimized Model Results

show hyperparam effects viz (iter and heatmaps)


### Model Comparisons

conclude
show performance results
show ROC curve

## Conclusions and Future Work

## References

https://cran.r-project.org/web/packages/rpart/rpart.pdf
https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf
https://stats.stackexchange.com/questions/191842/how-does-the-complexity-parameter-correspond-to-the-number-of-splits-in-cross-va




```{r}

spam.pred <- predict(spam.clf, newdata =  testDF)
listMeasures(spam.tsk)
performance(spam.pred, measures = list(auc, fpr, fnr, mmce))

# get scores
preds <- as.data.frame(spam.pred$data)
getPredictionProbabilities(spam.pred) # returns on prob for spam
# getPredictionResponse(spam.pred)
# getPredictionTruth()

# confusion matrix for default rpart
calculateConfusionMatrix(spam.pred)
calculateConfusionMatrix(spam.pred, relative = TRUE)


# for viz purposes, log top 2 importance, fit and predict
trainDF$perCaps <- log(1+trainDF$perCaps)
trainDF$bodyCharCt <- log(1+trainDF$bodyCharCt)

# obviously a lot of error when just using the top two predictors
# needs more to be accurate!
spam.log.tsk = makeClassifTask(id = "spam", 
                               data = trainDF, 
                               target = "isSpam")

g <- plotLearnerPrediction(learner = spam.lrn, 
                           task = spam.log.tsk, 
                           features=c("perCaps", "bodyCharCt"),
                           pointsize = 0.5, 
                           err.col="white",
                           bg.cols = c("darkblue","green"),
                           err.size = 0.5,
                           err.mark="cv")

g+
  ggtitle('Figure XX: Body Character Count and Percent Capitals')

# what does our optimal threshold look like for our metrics?
n = getTaskSize(spam.tsk)
thresh_plot = generateThreshVsPerfData(spam.pred, measures = list(auc, mmce))
plotThreshVsPerf(thresh_plot)


# RESAMPLING
spam.resamp = makeResampleDesc(method = "CV", 
                               iters = 10, 
                               stratify = TRUE)

# pull out resampling instance for reproducibility
#spam.resamp.bin = makeResampleInstance(spam.resamp, task=spam.tsk)
#save(spam.resamp.bin, file ="resampling_binary") # save for future use!
load("resampling_binary")


# TUNING 
# identify search grid params - we'll brute force here based on reasonable vals
# can also random search over numeric ranges instead
# test individual params (if range makeNumericParam(param, lower=, upper=))
spam.ps = makeParamSet(
                  makeDiscreteParam("cp", values = c(0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.5)),
                  makeDiscreteParam("minsplit", values = c(1, 5, 10, 15, 20, 30, 40)),
                  makeDiscreteParam("maxdepth", values = c(2, 5, 7, 10, 15, 20, 30, 40)),
                  makeDiscreteParam("parms", values = list(gini = list(split = c("gini")), 
                                                           info = list(split = c("information"))))
                 )

print(spam.ps)
# Create the grid and identify algorithm

# set up control, no params passed as we are searching
# over a discrete set of values
spam.ctrl = makeTuneControlGrid() 

# launch parallel multicore for tuning
parallelStart(mode="multicore", cpus = 4)

# tune parameters for our learner, task, optimize for auc
spam.tuned.clf = tuneParams( learner = spam.lrn, 
                             task = spam.tsk, 
                             resampling = spam.resamp.bin, # compare to same resampling method!
                             control = spam.ctrl, 
                             par.set = spam.ps, 
                             measures = list(auc))
parallelStop()

# hyperparam effects
data <- generateHyperParsEffectData(spam.tuned.clf, 
                                    partial.dep = TRUE)

# partial dependency plots based on random forest regression
plotHyperParsEffect(data, x = "iteration", y = "auc.test.mean",
  plot.type = "line", partial.dep.learn = "regr.randomForest")

plotHyperParsEffect(data, x = "cp", y = "maxdepth", z = "auc.test.mean",
  plot.type = "heatmap", partial.dep.learn = "regr.randomForest")

plotHyperParsEffect(data, x = "minsplit", y = "maxdepth", z = "auc.test.mean",
  plot.type = "heatmap", partial.dep.learn = "regr.randomForest")


# get optimal hyperparameters for use on test
spam.opt.lrn <- setHyperPars(spam.lrn, par.vals = spam.tuned.clf$x)
spam.opt.clf <- mlr::train(spam.opt.lrn, spam.tsk)

# results
spam.opt.preds <- predict(spam.opt.clf, newdata = testDF)
spam.preds <- predict(spam.clf, newdata =  testDF)

performance(spam.preds, measures=list(auc, mmce))
performance(spam.opt.preds, measures=list(auc, mmce))


df = generateThreshVsPerfData(list(base = spam.preds, 
                                   optimized = spam.opt.preds), 
                                   measures = list(fpr, tpr))

# whooped that a**
plotROCCurves(df)
qplot(x = fpr, y = tpr, color = learner, data = df$data, geom = "path")

```

```{r dtree_explore, include=FALSE}
# this block is exploration of the mlr package in R
# optimize both fnr and fpr
spam.ctrl = makeTuneMultiCritControlGrid()

library(emoa)
spam.tuned.clf = tuneParamsMultiCrit( learner = spam.lrn, 
                                      task = spam.tsk, 
                                      resampling = spam.resamp.bin, # compare to same resampling method!
                                      control = spam.ctrl, 
                                      par.set = spam.ps, 
                                      measures = list(fnr, fpr))

df <- as.data.frame(trafoOptPath(spam.tuned.clf$opt.path))
df[df$fnr.test.mean+df$fpr.test.mean == min(df$fnr.test.mean+df$fpr.test.mean),] # use this to fit
lrn = setHyperPars(makeLearner("classif.rpart"), par.vals = list(cp=0.001, minsplit=5, maxdepth=15))


# get a list of preds for each fold
getRRPredictionList(spam.base.clf)
# get all ze data!
pred <- getRRPredictions(spam.base.clf)
head(pred$data)

# get individual model fits, turn on models = TRUE in resample()
spam.base.clf$models

# establish train and test (learning curve here?)
# 80/20 split works best
spam.tsk = makeClassifTask(id = "spam", 
                           data = df$ximp, 
                           target = "isSpam", 
                           check.data = FALSE)

spam.lrn = makeLearner("classif.rpart",  # use rpart algorithm, gini index is default for splitting
                        id ="spam", # give it an id
                        fix.factors.prediction = TRUE, # control for missing class if any)
                        predict.type = 'prob') 

r <- generateLearningCurveData(
  learners = spam.opt.lrn,
  task = spam.tsk,
  percs = seq(0.1, 1, by = 0.1),
  measures = list(auc, setAggregation(auc, train.mean)),
  resampling = makeResampleDesc(method = "CV", iters = 10, stratify = TRUE, predict="both"),
  show.info = FALSE)

plotLearningCurve(r, facet="learner")

# try some nested resampling for unbiased error assessment
# on an outer CV, use inner to tune params, find best
# train model on outer CV splits based on these params and test on outer CV split
parallelStart(mode="multicore", cpus = 4)

tune.lrn = makeTuneWrapper(learner = spam.lrn,
                           resampling = spam.resamp, # inner resampling loop
                           par.set = spam.ps, # parameters to test
                           control = spam.ctrl, # control
                           measures = auc,
                           show.info = FALSE)

# unbiased test outer cross validation
outer = makeResampleDesc("CV", iters = 3, stratify = TRUE)

r = resample(tune.lrn, 
             spam.tsk, 
             resampling = outer, 
             extract = getTuneResult,
             measures = auc,
             show.info = FALSE)

parallelStop()

getNestedTuneResultsX(r)
data <- generateHyperParsEffectData(r, partial.dep = TRUE)
plotHyperParsEffect(data[[1]], x = "iteration", y = "auc.test.mean",
                    plot.type = "line", partial.dep.learn = "regr.randomForest")


```

```{r nbayes_get_data, include=FALSE}
# RSpamData not available in R 3.4.3

spamPath <- "Data"

dirNames <- list.files(path = spamPath, full.names = TRUE)

fileNames <- list.files(dirNames[1], full.names = TRUE)

sapply(dirNames, function(x) length(list.files(x)))

# pick random emails
idx = c(1:5, 15, 27, 68, 69, 329, 404, 427, 516, 852, 971)

#sampleEmail = sapply(list.files(dirNames[1], full.names = TRUE)[idx], readLines)

# split into header and body
splitMessage <- function(msg) {
  splitPoint = match("", msg) # find first blank line
  header = msg[1:(splitPoint-1)] # get header, will use this for attachments
  body = msg[ -(1:splitPoint) ] # get body
  return(list(header = header, body = body))
}

sampleSplit <- lapply(sampleEmail, splitMessage)

# get headers
headerList = lapply(sampleSplit, function(msg) msg$header)

# get boundary id from multi-part email

getBoundary<- function(header) {
  boundaryIdx = grep("boundary=", header) # index boundary
  boundary = gsub('"', "", header[boundaryIdx]) # replace quotes
  gsub(".*boundary= *([^;]*);?.*", "\\1", boundary) # regexrepl with first group match
}

# pass boundary and body of each message
dropAttach = function(body, boundary){
  bString = paste("--", boundary, sep = "") 
  bStringLocs = which(bString == body)# find start locs for each section of body
  # -- is arbitrary
  
  if (length(bStringLocs) <= 1) return(body) # if no boundaries, give me back body
  
  eString = paste("--", boundary, "--", sep = "") # ending boundary string
  eStringLoc = which(eString == body) # ending string loc
  if (length(eStringLoc) == 0)  # if no ending string return first part of body
    return(body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1)])
  
  # otherwise get length of body
  n = length(body)
  # if ending boundary string < rows, return body + ending rows after ending boundary
  if (eStringLoc < n) 
     return( body[ c( (bStringLocs[1] + 1) : (bStringLocs[2] - 1), 
                    ( (eStringLoc + 1) : n )) ] )
  
  # finally, return body if no other conditions met
  return( body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1) ])
}

# clean up punctuation, numbers, spaces
cleanText =
function(msg)   {
  tolower(gsub("[[:punct:]0-9[:space:][:blank:]]+", " ", msg))
}

findMsgWords = 
function(msg, stopWords) {
 if(is.null(msg))
  return(character())

  # split and return vector of cleaned words
 words = unique(unlist(strsplit(cleanText(msg), "[[:blank:]\t]+")))
 
 # drop empty and 1 letter words
 words = words[ nchar(words) > 1]
 words = words[ !( words %in% stopWords) ]
 invisible(words)
}

processAllWords = function(dirName, stopWords)
{
  # read all files in a given directory
  fileNames = list.files(dirName, full.names = TRUE)
  # drop files that are not email
  notEmail = grep("cmds$", fileNames)
  
  if ( length(notEmail) > 0) fileNames = fileNames[ - notEmail ]

  # read all messages into a list
  messages = lapply(fileNames, readLines, encoding = "latin1")
  
  # split header and body for each message
  emailSplit = lapply(messages, splitMessage)
  # put body and header in own lists
  bodyList = lapply(emailSplit, function(msg) msg$body)
  headerList = lapply(emailSplit, function(msg) msg$header)
  # save some memory, kill emailSplit var
  rm(emailSplit)
  
  # determine which messages have attachments
  hasAttach = sapply(headerList, function(header) {
    CTloc = grep("Content-Type", header)
    if (length(CTloc) == 0) return(0)
    multi = grep("multi", tolower(header[CTloc])) 
    if (length(multi) == 0) return(0)
    multi # has attachment
  })
  
  # get indices for which messages have attachments
  hasAttach = which(hasAttach > 0)
  
  # find boundary strings for messages with attachments
  boundaries = sapply(headerList[hasAttach], getBoundary)
  
  # drop attachments from message bodies
  bodyList[hasAttach] = mapply(dropAttach, bodyList[hasAttach], 
                               boundaries, SIMPLIFY = FALSE)
  
  # extract words from body
  msgWordsList = lapply(bodyList, findMsgWords, stopWords)
  
  invisible(msgWordsList)
}

msgWordsList = lapply(dirNames, processAllWords, stopWords=stopWords)

numMsgs = sapply(msgWordsList, length)

# identify which messages are spam based on num Msgs
isSpam = rep(c(FALSE, FALSE, FALSE, TRUE, TRUE), numMsgs)

# list of vectors of cleaned words for each message
msgWordsList = unlist(msgWordsList, recursive = FALSE)
```

``` {r nbayes_train_test_split, include=FALSE}
set.seed(418910)

numEmail = length(isSpam)
numSpam = sum(isSpam)
numHam = numEmail - numSpam

# take 1/3 of data for testing from each class spam ham
testSpamIdx = sample(numSpam, size =floor(numSpam/3))
testHamIdx = sample(numHam, size=floor(numHam/3))

# get word vectors from msgWordsList
# test
testMsgWords = c((msgWordsList[isSpam])[testSpamIdx],
                 (msgWordsList[!isSpam])[testHamIdx])

# train
trainMsgWords = c((msgWordsList[isSpam])[-testSpamIdx],
                 (msgWordsList[!isSpam])[-testHamIdx])

# get labels for train and test
testIsSpam = rep(c(TRUE, FALSE), 
                 c(length(testSpamIdx), length(testHamIdx)))

trainIsSpam = rep(c(TRUE, FALSE), 
                 c(numSpam - length(testSpamIdx), 
                   numHam - length(testHamIdx)))

```

```{r nbayes_freq_table, include=FALSE}

computeFreqs <- function(wordsList, spam, bow = unique(unlist(wordsList))) {
   # create a matrix for spam, ham, and log odds
  wordTable = matrix(0.5, nrow = 4, ncol = length(bow), # default value is 0.5
                     dimnames = list(c("spam", "ham",  # name dimensions
                                        "presentLogOdds", 
                                        "absentLogOdds"),  bow))

  # build frequency table 
  # For each spam message, add 1 to counts for words in message
  counts.spam = table(unlist(lapply(wordsList[spam], unique)))
  wordTable["spam", names(counts.spam)] = counts.spam + .5

   # Similarly for ham messages
  counts.ham = table(unlist(lapply(wordsList[!spam], unique)))  
  wordTable["ham", names(counts.ham)] = counts.ham + .5  


   # Find the total number of spam and ham
  numSpam = sum(spam) # from T/F vector passed to func
  numHam = length(spam) - numSpam

  # Prob(word|spam) and Prob(word | ham)
  # compute probabilities for each word given spam or ham
  wordTable["spam", ] = wordTable["spam", ]/(numSpam + .5)
  wordTable["ham", ] = wordTable["ham", ]/(numHam + .5)
  
   # calculate log odds
  # log odds of presence
  wordTable["presentLogOdds", ] = 
     log(wordTable["spam",]) - log(wordTable["ham", ])
  # log odds of absence
  wordTable["absentLogOdds", ] = 
     log((1 - wordTable["spam", ])) - log((1 -wordTable["ham", ]))

  invisible(wordTable)
}

```

```{r nbayes_classify, include=FALSE}

# use our frequency and odds table to classify new messages
computeMsgLLR = function(words, freqTable) 
{
       # Discards words not in training data.
  words = words[!is.na(match(words, colnames(freqTable)))]

       # Find which words are present
  present = colnames(freqTable) %in% words

  sum(freqTable["presentLogOdds", present]) +
    sum(freqTable["absentLogOdds", !present])
}

testLLR = sapply(testMsgWords, computeMsgLLR, trainTable)

tapply(testLLR, testIsSpam, summary)

pdf("SP_Boxplot.pdf", width = 6, height = 6)
spamLab = c("ham", "spam")[1 + testIsSpam]
boxplot(testLLR ~ spamLab, ylab = "Log Likelihood Ratio",
      #  main = "Log Likelihood Ratio for Randomly Chosen Test Messages",
        ylim=c(-500, 500))
dev.off()

typeIErrorRate = 
function(tau, llrVals, spam)
{
  classify = llrVals > tau
  sum(classify & !spam)/sum(!spam)
}

typeIErrorRate(0, testLLR,testIsSpam)

typeIErrorRate(-20, testLLR,testIsSpam)

typeIErrorRates = 
function(llrVals, isSpam) 
{
  o = order(llrVals)
  llrVals =  llrVals[o]
  isSpam = isSpam[o]

  idx = which(!isSpam)
  N = length(idx)
  list(error = (N:1)/N, values = llrVals[idx])
}

typeIIErrorRates = function(llrVals, isSpam) {
    
  o = order(llrVals)
  llrVals =  llrVals[o]
  isSpam = isSpam[o]
    
    
  idx = which(isSpam)
  N = length(idx)
  list(error = (1:(N))/N, values = llrVals[idx])
  }  

xI = typeIErrorRates(testLLR, testIsSpam)
xII = typeIIErrorRates(testLLR, testIsSpam)
tau01 = round(min(xI$values[xI$error <= 0.01]))
t2 = max(xII$error[ xII$values < tau01 ])

pdf("LinePlotTypeI+IIErrors.pdf", width = 8, height = 6)

library(RColorBrewer)
cols = brewer.pal(9, "Set1")[c(3, 4, 5)]
plot(xII$error ~ xII$values,  type = "l", col = cols[1], lwd = 3,
     xlim = c(-300, 250), ylim = c(0, 1),
     xlab = "Log Likelihood Ratio Values", ylab="Error Rate")
points(xI$error ~ xI$values, type = "l", col = cols[2], lwd = 3)
legend(x = 50, y = 0.4, fill = c(cols[2], cols[1]),
       legend = c("Classify Ham as Spam", 
                  "Classify Spam as Ham"), cex = 0.8,
       bty = "n")
abline(h=0.01, col ="grey", lwd = 3, lty = 2)
text(-250, 0.05, pos = 4, "Type I Error = 0.01", col = cols[2])

mtext(tau01, side = 1, line = 0.5, at = tau01, col = cols[3])
segments(x0 = tau01, y0 = -.50, x1 = tau01, y1 = t2, 
         lwd = 2, col = "grey")
text(tau01 + 20, 0.05, pos = 4,
     paste("Type II Error = ", round(t2, digits = 2)), 
     col = cols[1])

dev.off()

k = 5
numTrain = length(trainMsgWords)
partK = sample(numTrain)
tot = k * floor(numTrain/k)
partK = matrix(partK[1:tot], ncol = k)

testFoldOdds = NULL
for (i in 1:k) {
  foldIdx = partK[ , i]
  trainTabFold = computeFreqs(trainMsgWords[-foldIdx], trainIsSpam[-foldIdx])
  testFoldOdds = c(testFoldOdds, 
               sapply(trainMsgWords[ foldIdx ], computeMsgLLR, trainTabFold))
}

testFoldSpam = NULL
for (i in 1:k) {
  foldIdx = partK[ , i]
  testFoldSpam = c(testFoldSpam, trainIsSpam[foldIdx])
}

xFoldI = typeIErrorRates(testFoldOdds, testFoldSpam)
xFoldII = typeIIErrorRates(testFoldOdds, testFoldSpam)
tauFoldI = round(min(xFoldI$values[xFoldI$error <= 0.01]))
tFold2 = xFoldII$error[ xFoldII$values < tauFoldI ]

```
